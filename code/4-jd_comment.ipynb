{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jieba\n",
      "  Downloading jieba-0.42.1.tar.gz (19.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: jieba\n",
      "  Building wheel for jieba (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for jieba: filename=jieba-0.42.1-py3-none-any.whl size=19314458 sha256=d10a30536b12a50779fc3c55c605b651617cb01d828471bae358ab9ac60bd639\n",
      "  Stored in directory: /Users/liang/Library/Caches/pip/wheels/ca/38/d8/dfdfe73bec1d12026b30cb7ce8da650f3f0ea2cf155ea018ae\n",
      "Successfully built jieba\n",
      "Installing collected packages: jieba\n",
      "Successfully installed jieba-0.42.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始文本: Hello, world! 你好，世界！\n",
      "移除标点后: Hello world 你好世界\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    # 定义中英文常见标点符号\n",
    "    punctuation = r\"\"\"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~。，、；：？！…—～·《》「」『』（）〔〕【】〈〉\"\"\"\n",
    "    \n",
    "    # 使用 re.sub 函数替换掉文本中的标点符号\n",
    "    text_no_punctuation = re.sub(f\"[{re.escape(punctuation)}]\", \"\", text)\n",
    "    \n",
    "    return text_no_punctuation\n",
    "\n",
    "# 示例文本，包含中英文标点符号\n",
    "text = \"Hello, world! 你好，世界！\"\n",
    "\n",
    "# 移除标点符号\n",
    "text_no_punctuation = remove_punctuation(text)\n",
    "\n",
    "print(\"原始文本:\", text)\n",
    "print(\"移除标点后:\", text_no_punctuation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "good_file = './data/jd_comment/good.txt'\n",
    "bad_file = './data/jd_comment/bad.txt'\n",
    "\n",
    "def prepare_data(good_file, bad_file, filter=True):\n",
    "    all_words, pos_sentences, neg_sentences = [], [], []\n",
    "\n",
    "    # 定义处理每一行的内部函数\n",
    "    def process_line(line, sentence_list):\n",
    "        if filter:\n",
    "            line = remove_punctuation(line)\n",
    "        words = jieba.lcut(line.strip())\n",
    "        if words:\n",
    "            all_words.extend(words)\n",
    "            sentence_list.append(words)\n",
    "\n",
    "    def process_file(file_path, sentence_list):\n",
    "        with open(file_path, 'r') as f:\n",
    "            for line in f:\n",
    "                process_line(line, sentence_list)\n",
    "\n",
    "    process_file(good_file, pos_sentences)\n",
    "    process_file(bad_file, neg_sentences)\n",
    "\n",
    "    # 使用列表推导式和enumerate生成词典\n",
    "    dit = {word: [idx, freq] for idx, (word, freq) in enumerate(Counter(all_words).items())}\n",
    "\n",
    "    return pos_sentences, neg_sentences, dit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/wx/06p9f5fn4dqg83r5dzp31nmc0000gn/T/jieba.cache\n",
      "Loading model cost 0.304 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "pos_sentences, neg_sentences, dit = prepare_data(good_file, bad_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = sorted([(v[1], w) for w, v in dit.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2index(word, dit):\n",
    "    if word in dit:\n",
    "        return dit[word][0]\n",
    "    return -1\n",
    "\n",
    "def index2word(idx, dit):\n",
    "    if idx < len(dit):\n",
    "        return list(dit.items())[idx][0]\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2index('好', dit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'好'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index2word(19, dit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "dataset, labels = [], []\n",
    "\n",
    "def sentence_to_sample(samples, label, dit):\n",
    "\n",
    "    def sentence_to_vec(data, dit):\n",
    "        vector = np.zeros(len(dit))\n",
    "        for w in data:\n",
    "            vector[w] += 1\n",
    "        # 归一化\n",
    "        return (1.0 * vector / len(data))\n",
    "    \n",
    "    for sample in samples:\n",
    "        data = []\n",
    "        for w in sample:\n",
    "            if w in dit:\n",
    "                data.append(word2index(w, dit))\n",
    "        dataset.append(sentence_to_vec(data, dit))\n",
    "        labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_to_sample(pos_sentences, 0, dit)\n",
    "sentence_to_sample(neg_sentences, 1, dit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13031"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset, labels, test_size=0.1, random_state=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        return self.sigmoid(self.fc2(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, 训练损失：0.31691455947909436, 校验损失：0.247953397926034，校验准确率：0.9284615384615384\n",
      "Epoch: 2, 训练损失：0.2692962646058709, 校验损失：0.24875354953110218，校验准确率：0.9230769230769231\n",
      "Epoch: 3, 训练损失：0.24103465449381126, 校验损失：0.3327728551698894，校验准确率：0.9138461538461539\n",
      "Epoch: 4, 训练损失：0.22083395568473962, 校验损失：0.3729447650655014，校验准确率：0.9069230769230769\n",
      "Epoch: 5, 训练损失：0.20523681831431856, 校验损失：0.507118391861185，校验准确率：0.91\n",
      "Epoch: 6, 训练损失：0.19272438002343656, 校验损失：0.6380489940368911，校验准确率：0.8976923076923077\n",
      "Epoch: 7, 训练损失：0.18240509342538327, 校验损失：0.6108727773151746，校验准确率：0.9046153846153846\n",
      "Epoch: 8, 训练损失：0.1736917509320635, 校验损失：0.6402460298182943，校验准确率：0.9030769230769231\n",
      "Epoch: 9, 训练损失：0.16613386320580706, 校验损失：0.6543342751996001，校验准确率：0.9023076923076923\n",
      "Epoch: 10, 训练损失：0.15947608304582464, 校验损失：0.6781645623580893，校验准确率：0.9023076923076923\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "model = Net(len(dit), 32)\n",
    "cost = torch.nn.BCELoss()\n",
    "optim = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "epochs = 1\n",
    "records = []\n",
    "losses = []\n",
    "\n",
    "train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float), torch.tensor(y_train, dtype=torch.float))\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float), torch.tensor(y_val, dtype=torch.float))\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train() \n",
    "    for x, y in train_loader:\n",
    "        optim.zero_grad()\n",
    "        pred = model(x)\n",
    "        loss = cost(pred.squeeze(dim=1), y)\n",
    "        losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "    # 模型评估\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_losses = []\n",
    "        corrects = 0\n",
    "        for x, y in val_loader:\n",
    "            pred = model(x)\n",
    "            pval = (pred > 0.5).long()\n",
    "            cor = (pval.squeeze() == y.long()).sum().item()\n",
    "            corrects += cor\n",
    "            loss = cost(pred.squeeze(dim=1), y)\n",
    "            val_losses.append(loss.item())\n",
    "\n",
    "    acc = corrects / len(val_dataset)\n",
    "    print(f'Epoch: {epoch+1}, 训练损失：{np.mean(losses)}, 校验损失：{np.mean(val_losses)}，校验准确率：{acc}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.8923076923076924, loss: 0.5813928492276407\n"
     ]
    }
   ],
   "source": [
    "test_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float), torch.tensor(y_test, dtype=torch.float))\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "corrects = 0\n",
    "losses = []\n",
    "for x, y in test_loader:\n",
    "    pred = model(x)\n",
    "    pval = (pred > 0.5).long()\n",
    "    cor = (pval.squeeze() == y.long()).sum().item()\n",
    "    corrects += cor\n",
    "    loss = cost(pred.squeeze(dim=1), y)\n",
    "    losses.append(loss.item())\n",
    "\n",
    "print('acc: {}, loss: {}'.format(corrects / len(test_dataset), np.mean(losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
