我们继续来聊词向量，今天我们会亲自动手实现一下这个过程。



公众号`Coder梁`后台回复`ai`，无魔法无限使用GPT4，我个人用了好几个月了，真诚推荐。



## 数据和环境准备



本篇文章所有涉及的数据和源码都可以在我的github中找到：`https://github.com/moutsea/gpt_era_learn_algo`



首先，我们需要安装`jieba`这个库，这是一个业内比较有名的中文分词库。感兴趣的话，可以看下GPT对它的介绍。



![](https://moutsea-blog.oss-cn-hangzhou.aliyuncs.com/image-20240511074115201.png)



安装好了库之后，我们需要开始准备训练语料。



其实网上开源的中文语料不多，但也不是完全无迹可寻。我在github里找到了一份开源的中文语料库，但我试了一下，遇到了一些问题。



![](https://moutsea-blog.oss-cn-hangzhou.aliyuncs.com/image-20240512084015592.png)

首先是可用性的问题，我先是下载了里面体积最小的微博数据。下载了之后发现它完全没有经过任何预处理，没办法直接使用。比如保留了大量@和转发的`\\`等字段，还有emoji表情、图片等信息。



要将它清洗干净是一个巨大的工程，而中文的维基百科数据也同样存在类似的问题。需要做专门的预处理和清洗，我调研了一下，网上有开源的代码可以使用，但门槛同样不低。



最后是文本容量的问题，对于我们学习目的来说，这些语料库的体积还是太大了。要在这样规模的语料库上训练出一个收敛的词向量需要大量计算资源，个人是很难有这样的条件的。



其实，如果只是出于尝试和学习的目的，选什么语料库并不重要。重要的不是得到的词向量的结果，而是这个中间的过程。



所以最后我选了妇孺皆知的名著《西游记》的原文当做语料库，我把文本数据都放在了github里。大家感兴趣的话，也可以替换成其他的小说或者文章。



## 预处理



读取文件：



```python
with open('./data/nplm/xiyouji.txt', 'r', encoding='utf-16') as f:
    txt = str(f.read())
```



这里有一个小细节，这里我们使用的`encoding`是`utf-16`而不是`utf-8`。老实讲我之前没遇到过`utf-16`的格式，问了下GPT，它告诉我说`utf-16`在非拉丁文字中表现得更好，比如中文。



![](https://moutsea-blog.oss-cn-hangzhou.aliyuncs.com/image-20240512085052965.png)

输出一下开头，看到汉字，说明是读取成功了。

![](https://moutsea-blog.oss-cn-hangzhou.aliyuncs.com/image-20240512085124457.png)

接着使用`jieba`来对原文进行分词。



```python
import jieba

temp = jieba.lcut(txt)
```



预览一下，可以发现除了词语之外，还有很多标点符号。



![](https://moutsea-blog.oss-cn-hangzhou.aliyuncs.com/image-20240512085211163.png)



这些符号是用不到的，需要去掉。`string`库中的`punctuation`为我们提供了一些常用的英文标点符号，我们再加入中文的符号，进行过滤就得到了干净的词表。



```python
import string

punctuation = set(string.punctuation)
punctuation.update(['，', '。', '！', '？', '、', '；', '：', '“', '”', '‘', '’', '（', '）', '《', '》', '……', '—', '\u3000'])

def clean_text(text):
    # 使用 jieba 进行分词
    words = jieba.lcut(text)
    # 过滤掉标点符号和空白字符
    filtered_words = [word for word in words if word not in punctuation and not word.isspace()]
    return filtered_words

words = clean_text(txt)
```



可以看到全书一共有49212个词语。



![](https://moutsea-blog.oss-cn-hangzhou.aliyuncs.com/image-20240512085526681.png)



接着我们给每个词语赋予一个`id`，保证不同词的`id`是唯一的，在此过程中可以顺便统计一下词频。



```python
word_to_idx = {} 
idx_to_word = {}
ids = 0

for w in words:
    cnt = word_to_idx.get(w, [ids, 0])
    if cnt[1] == 0:
        ids += 1
    cnt[1] += 1
    word_to_idx[w] = cnt
    idx_to_word[ids] = w
```



每个词语对应一个`list`，第一个数表示`id`，第二个数表示词频。



![](https://moutsea-blog.oss-cn-hangzhou.aliyuncs.com/image-20240512085717731.png)



有了词表之后，我们就可以来生成训练数据了。这里我们使用的是`NGram`的方式，即通过前`N`个单词来预测下一个单词。这里我们选择的`N=2`，所以输入就是两个单词，输出是一个单词。



我们以滑动的方式遍历分词结果。



```python
trigrams = [([words[i], words[i + 1]], words[i + 2]) for i in range(len(words) - 2)]
indexed_trigrams = [([word_to_idx[words[i]][0], word_to_idx[words[i + 1]][0]], word_to_idx[words[i + 2]][0]) for i in range(len(words) - 2)]
```



数据完成之后，我们再用它实现一个`Dataset`，用来加工成模型的输入。



逻辑很简单，就是从原始数据中读入，然后转化成`tensor`。



```python
class NGramDataset(Dataset):
    def __init__(self, trigram_data):
        self.trigrams = trigram_data

    def __len__(self):
        return len(self.trigrams)

    def __getitem__(self, idx):
        context, target = self.trigrams[idx]
        context_idxs = torch.tensor(context, dtype=torch.long).to(device)
        target_idx = torch.tensor([target], dtype=torch.long).to(device)
        return context_idxs, target_idx
```





## 模型



模型实现的部分很简单，这就是一个简单的多分类模型，输入是`N`个单词，我们将它们转化成向量，再左右一个多分类的预测。



```python

class NGram(nn.Module):

    def __init__(self, vocab_size, embedding_dim, context_size):
        super(NGram, self).__init__()
        self.embeddings = nn.Embedding(vocab_size, embedding_dim)  #嵌入层
        self.linear1 = nn.Linear(context_size * embedding_dim, 128) #线性层
        self.linear2 = nn.Linear(128, vocab_size)

    def forward(self, inputs):
        embeds = self.embeddings(inputs)
        embeds = embeds.view((embeds.size(0), -1))

        out = self.linear1(embeds)
        out = F.relu(out)
        out = self.linear2(out)
        log_probs = F.log_softmax(out, dim = 1)
        return log_probs
    
    def extract(self, inputs):
        embeds = self.embeddings(inputs)
        return embeds
```



这里有几个小细节，首先是`Embedding`组件，这个组件我们之前没有用过，它是专门用来生成向量的。



`nn.Embedding(vocab_size, embedding_dim)  `的第一个输入`vocab_size`表示词表的大小，即不同单词的数量，也就是需要生成的向量的数量，第二个输入表示每个向量的长度。我们也可以传入更多参数对初始化方式进行指定，默认是随机初始化，也就是一开始向量的取值是随机的。



第二个细节是`log_probs = F.log_softmax(out, dim = 1)`，这里我们没有用`softmax`而是用了`log_softmax`。这是基于`softmax`的一个优化，可以解决`softmax`在计算时不稳定的问题。这个问题我们在专题的第14篇中讨论过，大家感兴趣可以翻阅一下。



这些都搞定之后，我们就可以训练模型了。整体的训练代码和之前的几乎没什么差别，并且由于不需要验证和测试，要简单很多。



```python
import os

model = NGram(len(vocab), 16, 2)

model_path = os.path.join('../resource/word2vec', 'word2vec.pt')

model.load_state_dict(torch.load(model_path))
model.to(device)

criterion = nn.NLLLoss()
optimizer = optim.SGD(model.parameters(), lr=0.001)

epochs = 2000
batch_size = 64

dataset = NGramDataset(indexed_trigrams)

data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

for epoch in range(epochs):
    running_loss = 0.0
    total = 0

    for X, labels in data_loader:
        outputs = model(X)

        optimizer.zero_grad()
        loss = criterion(outputs, labels.squeeze())

        loss.backward()
        optimizer.step()

        running_loss += loss.cpu().item() * labels.size(0)
        total += labels.size(0)   

    print(f'epoch: {epoch}, loss: {running_loss / total}')

torch.save(model.state_dict(), model_path)

```



不过有一点要说的是，模型收敛得很慢，我花了一整天的时间训练了差不多两千个epoch都没有完全收敛……



## 降维和展示



词向量训练好了之后，我们怎么知道效果如何呢？



大概有两种方法，一种是将这些向量投影在二维平面，这样我们就可以直观地在平面上看到这些向量之间的关系了。当然这样做会导致一些信息损失，因为我们训练之后得到的是16维的向量，而压缩到2维之后，会损失大量信息。不过，当做一个直观的参考还是可以的。



我们可以使用`PCA`算法来对向量进行降维，`PCA`是机器学习领域常用的一种对高维向量进行降维的方式，可以尽可能地保留数据的原始信息。我们将原本16维的向量降低到2维，相当于在平面上进行投影。



`sklearn`库中已经集成了相关用法，我们并不需要额外再费力实现。



```python
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import matplotlib
import os

model = NGram(len(vocab), 16, 2)
model_path = os.path.join('../resource/word2vec', 'word2vec.pt')
# 加载模型参数
model.load_state_dict(torch.load(model_path))
model.to(device)

vec = model.extract(torch.tensor([v[0] for v in word_to_idx.values()], dtype = torch.long).to(device))
vec = vec.cpu().data.numpy()

# 利用PCA算法进行降维
X_reduced = PCA(n_components=2).fit_transform(vec)
```



降维完成之后，我们再使用`matplotlib`库将它展示出来。



这里我们用到了一些新的函数，如果好奇它们用法的话可以询问GPT，它会返回详细的使用方法。



我们可以认为设置几个需要高亮的向量，来直观地感受到向量的信息。但这里有一个小细节，因为要在图片中展示中文，所以需要指定中文字体，否则会导致乱码无法显示。



```python
fig = plt.figure(figsize = (30, 20))
ax = fig.gca()
ax.set_facecolor('white')
ax.plot(X_reduced[:, 0], X_reduced[:, 1], '.', markersize = 1, alpha = 0.4, color = 'black')

words = ['悟空', '八戒', '悟净', '观音菩萨', '如来', '罗汉', '玉帝', '大圣','唐三藏', '和尚', '道人', '星君', '青牛', '王母', '神通', '星宿', '菩萨', '妖怪', '嫦娥', '天蓬', '卷帘']

# 指定字体，防止中文无法显示
zhfont1 = matplotlib.font_manager.FontProperties(fname='./data/nplm/HuaWenFangSong-1.ttf', size=22)

for w in words:
    if w in word_to_idx:
        ind = word_to_idx[w][0]
        xy = X_reduced[ind]
        plt.plot(xy[0], xy[1], '.', alpha =1, color = 'red')
        plt.text(xy[0], xy[1], w, fontproperties = zhfont1, alpha = 1, color = 'black')
        
plt.show()
```



最后，我们得到了这样一张图：



![](https://moutsea-blog.oss-cn-hangzhou.aliyuncs.com/image-20240512195952597.png)



我标记了一些人物的向量，可以看到他们之间还是存在一定的空间上的邻近关系的。



比如除了玉帝之外，佛教势力基本上都在左上上，而道教势力则在右下方。左上方中还可以明显看到取经团队之间比较接近。道教势力当中卷帘大将、嫦娥和天蓬元帅之间的距离也很接近，毕竟取经之前都是在一起上班的，都是老熟人了。



除了降维画图之外，还有一种方法就是直接根据向量之间的距离寻找最接近的向量。



```python
import numpy as np

# 定义计算cosine相似度的函数
def cos_similarity(vec1, vec2):
    
    norm1 = np.linalg.norm(vec1)
    norm2 = np.linalg.norm(vec2)
    norm = norm1 * norm2
    dot = np.dot(vec1, vec2)
    result = dot / norm if norm > 0 else 0
    return result
    
# 在所有的词向量中寻找到与目标词（word）相近的向量，并按相似度进行排列
def find_most_similar(word, vectors, word_idx, k):
    vector = vectors[word_to_idx[word][0]]
    simi = [[cos_similarity(vector, vectors[num]), key] for num, key in enumerate(word_idx.keys())]
    sort = sorted(simi)[::-1]
    words = [i[1] for i in sort]
    return words[: k]

```



但很遗憾，我试了一下，除了三藏和唐僧这一对之外，没再找到什么强关联的词语对了。



![](https://moutsea-blog.oss-cn-hangzhou.aliyuncs.com/image-20240512200732062.png)



只能说有一定效果，但还没有很好。



效果没有很好是正常的，一方面模型没有完全收敛，还有提升的空间。另外一方面，分词的效果其实就不够理想。《西游记》是古典小说，很多遣词造句都偏文言，`jieba`分词的效果平平，这也对最终效果带来很大影响。



我们使用的算法本身也有提升的空间，`NGram`还是有些过于简单粗暴了，尤其是我们的选取的`N=2`也很小。算法的优化同样可以带来效果的提升。



那除了`NGram`之外还有哪些词向量的生成方法呢？



稍安勿躁，会在下篇文章和大家继续分享。



最后，宣传一下我的星球。



除了Ai什么都有，目前在更新《穷爸爸与富爸爸》读书笔记，欢迎加入讨论



![](https://moutsea-blog.oss-cn-hangzhou.aliyuncs.com/image-20240404143631289.png)