今天我们来实际实现一下线性回归模型。



## 安装环境



我这里的例子参考李沐大神的《动手学深度学习》，所以我们需要安装一下动手学相关的包`d2l`。



```bash
pip install d2l
```



有一个小问题需要注意，直接安装往往会遇到权限问题，如果是`macOS`或者是`linux`系统，那么安装的时候需要`sudo`一下。

![](https://moutsea-blog.oss-cn-hangzhou.aliyuncs.com/image-20240129085445159.png)


如果是windows系统的话，需要右键打开终端，选择以管理员身份运行。



## 线性回归



线性回归的本质就是使用线性方程来完成回归任务。



而线性方程，如果大家学过线性代数应该非常熟悉。如果没有，也没关系，本质上线性方程就是`多元一次方程`。下图中是维基百科对线性方程的解释，不懂的小伙伴可以仔细阅读一下，或者可以询问ChatGPT。



![](https://moutsea-blog.oss-cn-hangzhou.aliyuncs.com/image-20240129085724520.png)



在机器学习当中，线性回归模型的方程一般写作：


$$
y = Xw +b
$$


这里的$X$是一个`(n, m)`的矩阵，`n`表示样本行数，`m`表示特征的数量。`w`是权重向量，`shape`是`(m, 1)`。所以$Xw$相乘之后的`shape`为`(n, 1)`，$b$是一个常数。



在线性回归问题当中，$X$和$y$都是事先采集好的，我们要做的就是通过梯度下降算法寻找最优的$w$和$b$。



在本文当中， 我们会从零开始，用最基础的代码实现一遍，带大家体会一下这当中的逻辑和流程。



## 数据生成



在实际应用场景当中，搜集数据、特征处理是一个非常大的工程。基本上对于一个算法工程师来说，80%的时间都花在这里。真正编写、调试模型所花的时间，只有20%不到。



但现在，我们只是为了学习的目的，可以不用特地去真实的场景搜集数据，而采用引入随机噪音的方法生成。



```python
def synthetic_data(w, b, num_examples): 
    """生成y=Xw+b+噪声"""
    X = torch.normal(0, 1, (num_examples, len(w))) # torch.norm接受的参数：均值，标准差，shape
    y = torch.matmul(X, w) + b
    y += torch.normal(0, 0.01, y.shape)
    return X, y.reshape((-1, 1))
```



我们调用这个函数创建数据：



```python
true_w = torch.tensor([2, -3.4]) #真实的w
true_b = 4.2  # 真实的b
features, labels = synthetic_data(true_w, true_b, 1000)
```



我们可以调用可视化工具，将这些数据制作成图，我们就能比较清晰地看到数据的分布情况了。



`d2l`当中封装了`matplotlib`库的部分用法，我们也可以自己调用`matplotlib`完成绘图，效果是一样的。感兴趣的同学可以询问一下ChatGPT关于`matplotlib`的基本用法。



```python
d2l.set_figsize()
d2l.plt.scatter(features[:, (1)].detach().numpy(), labels.detach().numpy(), 1)
```



![](https://moutsea-blog.oss-cn-hangzhou.aliyuncs.com/image-20240129093803640.png)



可以看到，虽然引入了噪音数据，但是这些数据的分布还是大体上遵循线性方程的。



## 训练过程



数据搞定了，我们就可以开始准备训练模型了。



要训练模型，首先需要读取训练数据。



在真实场景当中， 训练数据的量往往是非常庞大的，对于一些规模较大的公司来说，都是以TB为单位的。这么大规模的训练数据显然是**不可能全部读入内存**当中喂给模型的，那么对数据集进行拆分就是必要的了。



这种对数据集进行拆分成多个批次，每次只选择一个批次进行训练迭代的方式称为**批量梯度下降**。



通常我们会敲定一个批次中样本的数量，每次迭代时都以随机的方式进行无放回地抽样，当所有样本都被抽中，即被认为完成了一轮的训练。一个批次叫做`batch`，对应的一个批次的样本数量叫做`batch_size`，一整轮训练叫做一个`epoch`。



一般情况下我们只会关心`epoch`的数量和`batch_size`的大小，而不会关心一个`epoch`中包含的`batch`的数量。



接下来我们来实现一个迭代器，每次都从训练集中抽取一个`batch`的数据进行返回。这场景里为了简化，我们没有保证样本是无放回的，在真实当中，通常都是无放回的抽样。



```python
def data_iter(batch_size, features, labels):
    num_examples = len(features)
    indices = list(range(num_examples))
    # 这些样本是随机读取的，没有特定的顺序
    random.shuffle(indices)
    for i in range(0, num_examples, batch_size):
        batch_indices = torch.tensor(indices[i: min(i + batch_size, num_examples)])
        yield features[batch_indices], labels[batch_indices]
```



因为是迭代器，我们可以直接和Python中的循环进行结合：



```python
batch_size = 10
for X, y in data_iter(batch_size, features, labels):
    print(X, '\n', y)
    break
```



## 模型训练



数据准备妥当之后，接下来就需要实现模型了。



根据我们前面列出的公式，我们只需要两个参数，即$w$和$b$。它们的`shape`我们已经知道了，我们直接用`tensor`把它们表示出来：



```python
w = torch.normal(0, 0.01, size=(2,1), requires_grad=True)
b = torch.zeros(1, requires_grad=True)
```



参数有了，模型自然也就有了：



```python
def linreg(X, w, b):
    """线性回归模型"""
    return torch.matmul(X, w) + b
```



模型有了之后，我们来实现损失函数：



```python
def squared_loss(y_hat, y): 
    """均方损失"""
    return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2
```



因为是回归模型，所以我们使用的均方差作为损失函数，公式为：



$$loss = \frac 1 2 \sum_{i=1}^n(y_i - \hat{y}_i)^2$$



代码中的`y_hat`和`y`都是`tensor`，所以我们可以直接对它们进行操作，而不需要使用循环一个一个遍历了。



最后，我们还需要优化算法。



所谓的优化算法，即模型参数更新的算法。在上篇文章当中我们介绍过，是梯度下降算法，这是总的方法。但基于梯度下降算法还衍生出了许多优化，这些优化算法各有千秋，没有最优一说，需要根据实际情况酌情选择。



这里我们一切从简，只使用最基础的梯度下降算法。



由于我们基于Pytorch框架实现，所以**梯度并不需要我们手动计算**，Pytorch框架会自动帮我们计算好每个`tensor`的梯度，存入`grad`当中。所以我们要做的很简单，就是根据参数的梯度来迭代更新参数的值。



```python
def sgd(params, lr, batch_size):
    """小批量随机梯度下降"""
    with torch.no_grad():
        # torch.no_grad() 用于防止梯度跟踪
        for param in params:
            param -= lr * param.grad / batch_size
            # 迭代完成之后，将梯度清零
            param.grad.zero_()
```



代码当中我们引入了一个参数叫做`lr`，它是`learning rate`即学习率的缩写。它用来控制每次迭代的幅度，也可以理解成步长。



步长并不是越大越好，步长过大可能会导致模型错过局部最优解无法收敛。但也不是越小越好，步长过小会导致迭代速度过慢，训练时间过长。



![](https://wenwender.files.wordpress.com/2019/05/54520683_2388585747818232_1869223815087652864_n-1.jpg)



关于`lr`的设置也没有一个通用的办法，更多地还是基于经验。



## 训练



接下来就到了紧张刺激的时刻了，我们要把刚刚实现的功能全部串联在一起，实现模型的训练。



这段代码实现难度不大，逻辑也很简单，但是它非常重要。我们之后每次训练模型，基本上都会要用到。虽然每次的实现细节略有不同，但大体上都遵循这个框架。



大体上的流程就是：



- 获取一个`batch`的数据
- 丢给模型计算出`y_hat`
- 计算loss
- 计算梯度更新模型



```python
lr = 0.03
num_epochs = 3
net = linreg
loss = squared_loss
losses = []

for epoch in range(num_epochs):
    for X, y in data_iter(batch_size, features, labels):
        l = loss(net(X, w, b), y) # X和y的小批量损失
        # 因为l形状是(batch_size,1)，而不是一个标量。l中的所有元素被加到一起，
        # 并以此计算关于[w,b]的梯度
        l.sum().backward()
        sgd([w, b], lr, batch_size) # 使用参数的梯度更新参数
        with torch.no_grad():
            # 记录训练过程中的loss
            train_l = loss(net(features, w, b), labels)
            # print(f'epoch {epoch + 1}, loss {float(train_l.mean()):f}')
            losses.append(float(train_l.mean()))
```



这里由于我们的模型比较简单，所以只需要迭代3个`epoch`就足够了。如果是复杂的模型，会需要更多轮次的迭代，耗时也会更长。



最后我们查看一下`loss`曲线，`loss`越小，说明模型的预测值和真实值越接近。当然由于有噪声的存在，`loss`是不可能收敛到0的，只能接近0。



![](https://moutsea-blog.oss-cn-hangzhou.aliyuncs.com/image-20240129101842717.png)



当然，我们也可以人为计算一下$w$和$b$与我们一开始创建数据时用的参数的差异：



![](https://moutsea-blog.oss-cn-hangzhou.aliyuncs.com/image-20240129102638275.png)



可以看到非常接近0，证明我们的模型确实拟合得很好。



最后，欢迎加入我的星球~



除了Ai什么都有，个人学习成长、理财投资、量化交易心得，持续更新中



![](https://moutsea-blog.oss-cn-hangzhou.aliyuncs.com/%E6%98%9F%E7%90%83%E4%BC%98%E6%83%A0%E5%88%B8%20(6).jpeg)