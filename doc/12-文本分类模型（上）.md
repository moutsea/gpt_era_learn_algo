大家好，这篇文章我们来做一个简单的文本分类模型实战。


公众号`Coder梁`后台回复`ai`，获取无魔法无限使用GPT4.0的方式。


## 词袋模型



如果大家仔细阅读完之前的文章，相信不难理解一个很重要的结论——字符串无法直接作为神经网络的输入。



如果你对此还有疑问的话，可以参考一下下方GPT的回答。



![](https://moutsea-blog.oss-cn-hangzhou.aliyuncs.com/image-20240317221505226.png)



那么问题来了，既然字符串不能直接作为模型的输入，那么模型是怎么处理文本信息的呢？



关于这个问题，经过多次迭代之后，已经有了比较成熟的解决方案。不过显而易见的是，效果越好的方案自然也会越复杂。作为初学者，我们当然不首先追求效果，从易到难，先来看看比较简单直观的方法。



其实我们之前已经介绍过了一种方法`onehot`，它也可以用来处理文本的问题。


但它有明显的硬伤，除了太过稀疏之外，它对于句子或者文章没有很好的办法表达。因为句子和文章当中的单词数量并不是固定的，这就导致了我们需要不确定数量的多个`onehot`数组来表示一段文本。


不确定数量在模型中同样是不可接受的，模型的输入都必须是固定的。唯一的方法就是强行规定一个句子长度，不论句子长短，都强行用规定长度的多个`onehot`数组来表达。但这么干会进一步增加空间的浪费。



于是，在`onehot`基础上，计算学家们做了一些改进，提出了词袋模型，英文叫做Bag of words。



照惯例，我们先来看下GPT的定义：



![](https://moutsea-blog.oss-cn-hangzhou.aliyuncs.com/image-20240317231115323.png)



光看描述可能不太好理解，我们可以让它再给出一个例子：



![](https://moutsea-blog.oss-cn-hangzhou.aliyuncs.com/image-20240317231248791.png)



简单来说，它和`onehot`原理比较类似，只不过`onehot`当中只有一位为1，其余均为0，而在词袋模型当中则不然。


首先，不会限制大于0的数量，句子中不同的单词数量越多，大于0的维度也会越多。其次，也不会限制最大值，数组当中存储的是对应位置的单词的权重，单词出现的次数越多，权重越大。



词袋模型解决了句子难表示的问题，并且对于单词的权重表达也有很多变种。比如除了使用单词出现的频次作为权重之外，也可以使用`TF-IDF`等方式。



但词袋同样也有很多缺陷。比如由于词表通常较大，导致过于稀疏。另外由于词袋当中只会存储每个单词的权重，而忽略了单词之间的顺序关系，`狗咬了人`和`人咬了狗`使用词袋表示的结果是完全一样的，而在自然语言当中，这是两个完全不同的含义。



## 实战问题



这次我们实战的问题是电商用户评论情绪分类，数据来源于某东用户真实的评论，我会把数据上传到我的github当中，大家可以通过github去获取：`https://github.com/moutsea/gpt_era_learn_algo/tree/master/doc`



下面给大家截取几条数据：



![](https://moutsea-blog.oss-cn-hangzhou.aliyuncs.com/image-20240318223529870.png)



一共有两个`txt`文件，分别是`good.txt`存储了好评的评论和`bad.txt`存储了差评评论。



### 分词



我们注意到，这里的评论都是中文，中文相比于英文会多一个**分词**的问题。



由于中文的特殊性，**汉字在一句话中都是连续排列的**。并且词语和断句都没有限制，意味着我们很难在不进行词法、句法分析的情况下得到良好的分词结果。比如英文当中，单词之间都是以空格分隔的，因此不存在分词的问题，直接拆分句子即可。



中文的分词在NLP当中是一个比较重要的问题，并由此衍生了一系列算法。在机器学习时代之前，主要是基于匹配的算法，包括正向最大匹配、逆向最大匹配以及最少切分等算法。在机器学习兴起之后，我们基于概率和统计建立模型，有了更好的方法。



好在目前有许多开源的分词库供我们选择，所以我们并不需要从头开始设计分词算法。我们知道有这么个概念即可。



直接询问GPT就可以得到答案，免去了搜索的繁琐。



![](https://moutsea-blog.oss-cn-hangzhou.aliyuncs.com/image-20240318235120369.png)



这里我们选择GPT排第一的`jieba`，通过询问GPT可以知道，直接调用`lcut`函数即可得到中文句子的分词结果。



![](https://moutsea-blog.oss-cn-hangzhou.aliyuncs.com/image-20240318235207859.png)



### 正则过滤



除了分词之外，我们还需要处理句子中的标点符号。



这个功能我们可以基于正则表达式来实现，这个功能比较简单，我们直接询问GPT：



![](https://moutsea-blog.oss-cn-hangzhou.aliyuncs.com/image-20240319000424580.png)



这些准备工作都做完之后，我们就可以来准备进行数据预处理了。



首先我们来完成文件的读取，以及文本的分词。



不论是好评还是差评数据，它们都是样本的一部分，所以它们的处理逻辑是完全一样的，唯一不同的是它们的标签以及数据来源不同。



因此我们把中间的逻辑全部封装，通过输入参数来控制数据存储进不同的变量当中。



```python
from collections import Counter

good_file = './data/jd_comment/good.txt'
bad_file = './data/jd_comment/bad.txt'

def prepare_data(good_file, bad_file, filter=True):
    all_words, pos_sentences, neg_sentences = [], [], []

    def process_line(line, sentence_list):
        if filter:
            # 去除标点
            line = remove_punctuation(line)
        # jieba分词
        words = jieba.lcut(line.strip())
        if words:
            # 全量词表
            all_words.extend(words)
            sentence_list.append(words)

    def process_file(file_path, sentence_list):
        with open(file_path, 'r') as f:
            for line in f:
                process_line(line, sentence_list)

    process_file(good_file, pos_sentences)
    process_file(bad_file, neg_sentences)

    # 生成词表
    dit = {word: [idx, freq] for idx, (word, freq) in enumerate(Counter(all_words).items())}

    return pos_sentences, neg_sentences, dit
```



我们调用一下就可以得到词表以及正负样本了，基于词表我们还可以实现词语和`id`的互相转化：



```python
pos_sentences, neg_sentences, dit = prepare_data(good_file, bad_file)

def word2index(word, dit):
    if word in dit:
        return dit[word][0]
    return -1

def index2word(idx, dit):
    if idx < len(dit):
        return list(dit.items())[idx][0]
    return None
```



![](https://moutsea-blog.oss-cn-hangzhou.aliyuncs.com/image-20240319223627252.png)



到这里，我们基本上就做好了生成样本之前的所有准备工作了，接下来要做的就是根据词袋模型的定义来生成样本了。



```python
import numpy as np
dataset, labels = [], []

def sentence_to_sample(samples, label, dit):
    def sentence_to_vec(data, dit):
        vector = np.zeros(len(dit))
        for w in data:
            vector[w] += 1
        # 归一化
        return (1.0 * vector / len(data))
    
    for sample in samples:
        data = []
        for w in sample:
            if w in dit:
                # 将句子转化成index
                data.append(word2index(w, dit))
        # 将words对应的index数组使用词袋模型表达
        dataset.append(sentence_to_vec(data, dit))
        labels.append(label)
```


我们调用一下函数，传入正负样本的数据，就能得到训练样本了。

这里注意一下，由于我们的目标是预测用户给出的是不是差评，因此这里`neg_sentences`的`label`是1。


```python
sentence_to_sample(pos_sentences, 0, dit)
sentence_to_sample(neg_sentences, 1, dit)
```

模型以及实验的部分我们放到下一篇文章当中。


最后，宣传一下我的星球。



除了Ai什么都有，目前在更新《穷爸爸与富爸爸》读书笔记，欢迎加入讨论



![](https://moutsea-blog.oss-cn-hangzhou.aliyuncs.com/%E6%98%9F%E7%90%83%E4%BC%98%E6%83%A0%E5%88%B8%20(6).jpeg)