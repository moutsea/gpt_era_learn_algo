这一篇文章我们来聊词向量。



公众号`Coder梁`后台回复`ai`，无魔法无限使用GPT4，我个人用了好几个月了，真诚推荐。



其实关于词向量我们在之前的文章当中也曾经讨论过，不过当时我们是使用的朴素的`onehot`的方法来操作的。



我们当时曾经说了，这是一种已经过时的方法，现在已经没有人还在用了。因为我们已经有了更好的方法，这种方法就是`Word2vec`。它能构建出非常出色的词向量的表达，为后来NLP领域的发展，甚至是如今大模型的出现打下了基础，可想而知它的重要性。



## NPLM原理



表面上来看`Word2vec`只不过是一种将单词转化成向量，或者说根据文本生成向量的技术。但在它的背后，有着一套非常严谨且科学的理论体系，这套理论体系和GPT模型的原理是一以贯之的。这套体系基于概率论，称为概率语言模型。



我们都知道自然语言非常复杂，就比如大多数人都学过好几年的英语，但真正碰到外国人还是免不了抓瞎，明明背了几千个单词，但一句完整的句子都说不连贯。在我们人类眼里，自然语言是无比复杂的。



除了各种词句、用法、固定搭配之外，还有丰富且灵活的语法，还有很多生僻词，网络流行词等等。所以我们希望通过代码和显式的规则构建出这样一个语言模型几乎是不可能的，它的复杂度超过了我们的想象。



早年的时候关于如何构建语义模型也分为了两派，一派认为应该针对语法规则建模，通过构建详细的语法结构来提升模型的效果。另外一派认为应该走统计流，即不刻意构建语法，而是通过大量的语料让模型自己学习到其中的规律，即模仿人类学习语言的过程。



双方争执不下，经过一段时间的竞争之后，统计流派逐渐占据了上风，模型的泛化能力越来越强，在更多的任务和问题中展现了越来越好的效果。而相反，语法规则流则渐渐没落，如今已经几乎无人问津了。



关于这点有一个著名的笑话，说是早年双方还没有分出胜负的时候，有某个国际上著名的NLP团队分享了自己的经验心得：团队中每开除一名语言学家，模型的效果就会提升。



![](https://moutsea-blog.oss-cn-hangzhou.aliyuncs.com/image-20240510195253730.png)



统计学派的理论简单粗暴，一切都是概率。比如NLP中著名的N元语义模型，它做了一个大胆地假设：放弃所有语法限制，只考虑单词和单词之间的概率关系。认为句子中的每一个单词只和它之前的N个单词有关。


$$
p(w_{N+1} | w_1, w_2, ..., w_N) = \frac{count(w_1, w_2, ..., w_N, w_{N+1})}{count(w_1, w_2, ..., w_N)}
$$


比如说，小王迟到了，他被公司___。理论上来说，如果N足够大，并且模型见过的样本足够多，它就会知道在这句话后面出现惩罚的概率是最大的。



但同样，这里的N越大，模型的参数空间也就越大，那么收敛需要的样本就越多，模型的运算就越慢。



所以在实际使用中，N的取值往往比较小，比如最常见的就是2或者3。通常当N=3时，模型就已经足够在大多数场景中表现出色了。



当N=3时，也就是说我们认为在句子当中，每一个单词只和它之前的3个单词有关。也就是说我们希望模型学习这样一个函数：


$$
f(w_1, w_2, \cdots w_N)=w_{N+1}
$$
也就是根据前`N`个单词，预测出最有可能出现的下一个单词。



GPT模型能够根据我们的问题生成回答，看起来非常神奇，但其实内部也采用了类似的原理。模型会不停地根据之前的文本预测下一个最有可能出现的单词(token)。如此循环往复来生成的回答，并非是一口气一下子输出的。



## 模型结构



我们来设想一下训练好之后的`NGram`模型的结构，根据我们刚才的说明，它能够根据文本中连续的N个单词预测下一个单词。那么它的输入和输出分别是怎样的呢？或者说输入和输出的维度是多少呢？



可能一些人会想当然地以为模型有N个输入，一个输出。这N个输入就是这N个单词，一个输出就是下一个可能连接在这N个单词之后的单词。



但仔细想就会发现这是有问题的，我们人类在根据上文猜测下文的时候，我们大脑中除了有上文的信息之外，还有一个潜在的词表信息，即我们知道一共有哪些单词。映射到模型当中也是一样的，只有上下文信息是不够的，模型同样也需要词表信息，知道有哪些单词可选。



不过这个词表我们不是作为输入传进去的，而是直接集成在模型输出中的，模型在输出的时候，并不是凭空蹦出来的结果，而是预测了词表中每个单词可能出现的概率。所以模型的结构大概是这样的：



![](https://moutsea-blog.oss-cn-hangzhou.aliyuncs.com/image-20240427172019422.png)



详细观察一下上图，输入是`document`，它表示的是N个单词的组合。然后有一个中间层，我们假设中间层的维度是$d$，输出层的维度是$|V|+|F|$。这里的$V$指的是词表，而$F$指的是一些附加信息，比如类似停用词之类的标点符号的集合。



这个模型训练好了之后，看起来有些无趣，好像没什么特殊的用处。受制于模型的复杂度不高，它也不能生成什么像样的内容。



但突然有一天一位大佬突发奇想，如果我们训练好了模型之后，一次只输入一个单词的话，对于不同的单词，**中间层这$d$个节点的参数值似乎都是独一无二的**。



既然如此，我们能不能**利用这$d$个节点的参数值来反向表示这个单词呢**？也就是说，我们把原本一个自然语言中的词语转化成了一个数学上的向量。我们能不能把这个向量当做是这个单词的全部信息呢？



光猜当然是没用的，需要实际试一下。



经过实验之后，大家发现效果非常好。通过这种方式生成的向量展现出了许多特性，比如含义相近的单词转化成的向量在空间上也非常接近。



猫和狗虽然是不同的动物，但是它们在空间中的向量表示很近似。这很好理解，因为在句子当中它们常常是能够进行替换的。



> 比如我喜欢猫，因为它总是懒洋洋的。



把这句话中的猫换成狗，虽然宾语变了，但句子整体的意思也大差不差。



不仅如此，更神奇的是，这些向量还保留了一些向量本身的数学性质。比如我们用king和queen两个向量做差值，这个结果和man与woman的向量差值同样非常接近。也就是说`queen = woman + king - man`。



![](https://moutsea-blog.oss-cn-hangzhou.aliyuncs.com/word2vec-embedding-1.png)



后来，我们给通过这种方式生成的向量起了一个名字叫做`embedding`，这个词的中文翻译是嵌入的意思，但由于不是非常贴切的原因，业界一般不用嵌入这个中文词，依然还是使用embedding的英文，或者称为向量。将单词转化成embedding的方法也就叫做`Word2vec`。



时至今日，深度学习高速发展，我们早已经对各种东西的embedding不陌生。但回过头来看，这么一个小小创新的影响力是非常惊人的。无论是语言概率模型的思想还是将单词这种原本很难表达的内容转化成向量的方法，都奠定了之后自然语言处理领域乃至整个深度学习领域的基础。如今看起来不可思议的ChatGPT的最底层基石，同样是词向量和语言概率模型。



最后，宣传一下我的星球。



除了Ai什么都有，目前在更新《穷爸爸与富爸爸》读书笔记，欢迎加入讨论



![](https://moutsea-blog.oss-cn-hangzhou.aliyuncs.com/image-20240404143631289.png)

