大家好，今天我们继续上次的文本分类模型。



公众号`Coder梁`后台回复`ai`，获取无魔法无限使用GPT4.0的方式。



## 拆分数据集



在上一篇文章当中我们完成了数据预处理的部分，将用户的评论文本通过词袋模型处理成了向量。



![](https://p.ipic.vip/rdmdc8.png)



一共只有1.3w条数据，非常少，对于模型训练基本没什么压力。几乎完全没法发挥GPU的优势，没有GPU用CPU也是一样的。



接着我们需要把数据集拆分，拆分成训练集、验证集和测试集，然后构造`data_loader`。我们在之前的案例当中已经操作过了，不过多赘述了，直接上代码：



```python
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(dataset, labels, test_size=0.1, random_state=23)
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=23)

train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float), torch.tensor(y_train, dtype=torch.float))
train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)

val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float), torch.tensor(y_val, dtype=torch.float))
val_loader = DataLoader(val_dataset, batch_size=16)
```



当然，这里由于这个问题的数据集非常少，所以我们也可以不使用`batch_size`，针对样本进行逐条处理。



## 模型代码



```python
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F

class Net(nn.Module):
    def __init__(self, input_dim, hidden_dim):
        super(Net, self).__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, 1)
        self.relu = nn.ReLU()
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        x = self.relu(self.fc1(x))
        return self.sigmoid(self.fc2(x))
```



模型有关的代码可能是整个环节当中最简单的部分，唯一要注意的细节就是神经网络每一层的输入和输出的维度。需要保证上一层输出的维度和下一层输入的维度一致，好在在当前问题当中，没什么复杂的情况。



## 模型训练



实现好了模型，接下来要做的就是调用它来进行训练了。



这段代码和上次实战的其实大同小异，整体的框架都是一样的。最外层循环是`epoch`，表示训练中训练集迭代的次数。每一个`epoch`当中，则是通过`data_loader`对训练集进行遍历，得到的结果传入模型当中，得到预测值。再通过预测值和真实样本值来计算`loss`，最后通过优化算法来更新模型参数。



当一个`epoch`执行结束之后，在验证集上跑一下效果即可。有需要的话，再保存一下最优的模型参数和记录一些日志。



```python
import torch
from torch.utils.data import TensorDataset, DataLoader
import numpy as np

model = Net(len(dit), 32)
cost = torch.nn.BCELoss()
optim = torch.optim.Adam(model.parameters(), lr=0.01)
epochs = 1
records = []
losses = []

epochs = 10

for epoch in range(epochs):
    model.train() 
    for x, y in train_loader:
        optim.zero_grad()
        pred = model(x)
        loss = cost(pred.squeeze(dim=1), y)
        losses.append(loss.item())
        loss.backward()
        optim.step()

    # 模型评估
    model.eval()
    with torch.no_grad():
        val_losses = []
        corrects = 0
        for x, y in val_loader:
            pred = model(x)
            pval = (pred > 0.5).long()
            cor = (pval.squeeze() == y.long()).sum().item()
            corrects += cor
            loss = cost(pred.squeeze(dim=1), y)
            val_losses.append(loss.item())

    acc = corrects / len(val_dataset)
    print(f'Epoch: {epoch+1}, 训练损失：{np.mean(losses)}, 校验损失：{np.mean(val_losses)}，校验准确率：{acc}')

```



但是魔鬼都藏在细节里，这里面有很多细节值得关注，否则的话很容易出错。



比如，这里我们为什么要对`pred`执行`squeeze`操作？



![](https://p.ipic.vip/lqihqs.png)



在验证集当中又为什么需要用到类型转换？



![](https://p.ipic.vip/nbksch.png)



这两个问题我留给大家自己去思考，如果有疑惑的话，可以问GPT。



最后，我们看一下模型在测试集上的表现：



```python
test_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float), torch.tensor(y_test, dtype=torch.float))
test_loader = DataLoader(test_dataset, batch_size=16)

corrects = 0
losses = []
for x, y in test_loader:
    pred = model(x)
    pval = (pred > 0.5).long()
    cor = (pval.squeeze() == y.long()).sum().item()
    corrects += cor
    loss = cost(pred.squeeze(dim=1), y)
    losses.append(loss.item())

print('acc: {}, loss: {}'.format(corrects / len(test_dataset), np.mean(losses)))
```



![](https://p.ipic.vip/8tzh9l.png)



准确率接近90%，已经算是不错了，说明词袋模型在语义理解上有一定的表达能力。



对于剩下10%模型判断错误的样本，它们是什么情况呢？



我特地去看了一下，大概有三个主要原因。



一种是数据本身的问题，比如数据的`label`标错了，或者是样本质量过低，比如用户写的评论是一串火星文，根本没有任何有用的信息，或者是用户的评论过短，信息量不够等。这种情况占比并不多，不是主要的情况。



大部分情况还是对语义理解不够充分导致的，比如`质量不是很好`，模型的关注重点可能落在`好`上，从而将样本识别错误。这种情况就是词袋模型本身的欠缺导致的，因为它完全忽略字词之间的上下文关系，必然会导致模型理解能力欠缺。



要解决这个问题就必须要从词袋模型本身入手了，大家不要着急，我们会在之后NLP相关的内容继续讨论的。





最后，宣传一下我的星球。



除了Ai什么都有，个人学习成长、理财投资、量化交易心得，持续更新中



![](https://moutsea-blog.oss-cn-hangzhou.aliyuncs.com/%E6%98%9F%E7%90%83%E4%BC%98%E6%83%A0%E5%88%B8%20(6).jpeg)

