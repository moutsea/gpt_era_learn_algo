我们继续来聊词向量，这一篇文章我们来聊聊比NPLM更好的词向量算法。



公众号`Coder梁`后台回复`ai`，无魔法无限使用GPT4，我个人用了好几个月了，真诚推荐。



## NPLM的局限性



我们之前用来生成词向量的方法是NPLM，它基于语言概率建模，这个思想本身没有问题。通常也能得到比较好的效果，但有一个比较大的问题是效率。



通过NPLM方式构建的模型收敛速度较慢，需要花费大量的时间才能收敛。



之前我们使用《西游记》作为训练语料，足足训练了几千个epoch都没有完全收敛，可见它的效率之低。



为了解决这个问题，后续提出了新的训练词向量的模型，而后被广泛使用，一直延续至今。



这种新的方法于2013年提出，称作word2vec。它是通过上下文的形式来学习词向量的。



## word2vec



`word2vec`中包含两种方法，分别称为`skip-gram`和`CBOW`。



虽然说是两种方法，但是它们的思路非常接近，只是结构上有细微的差异。我们直接来看图理解，这是`CBOW`的模型结构：



![](https://moutsea-blog.oss-cn-hangzhou.aliyuncs.com/image-20240525161918489.png)



`CBOW`的原理是输入上下文预测中心单词，比如这里的$w(t)$是模型学习的目标，而模型的输入是它前后相邻的两个词语。



而`skip-gram`则与它相反，给定中心单词，预测前后的上下文。



![](https://moutsea-blog.oss-cn-hangzhou.aliyuncs.com/image-20240525162056846.png)



通过询问GPT得知在具体应用场景当中，`Skip-gram`要比`CBOW`更常见一些。



没办法毕竟现在`word2vec`已经非常成熟了，日常工作学习当中很少会遇到从头开始训练一个词向量的场景。



![](https://moutsea-blog.oss-cn-hangzhou.aliyuncs.com/image-20240527193757834.png)



`word2vec`本身的原理非常简单，也很直观，没有太多可以赘述的。不过有一些针对训练的优化值得一说，下面主要介绍两个比较常见的重大优化，分别是层次softmax和负采样。



在原版的模型当中，模型的输出一个维度是$|V|$的结果。



这里的$|V|$是词表的长度，通常对于一些较大的语料库，词表的长度动辄几十万甚至上百万。当我们使用反向传播进行训练的时候，对于其中的每一维都需要进行计算和更新。可想而知，这其中的计算规模。



为了加速收敛过程，势必就需要引入一些优化，减小计算的规模，从而提升收敛的速度。



## 层次softmax



第一种方法叫做层次softmax，也叫做层次归一化。



在原版的模型当中，我们的输出是一维的，长度是$|V|$的向量。很容易想到，如果我们将它增加一些层次变成复杂一些的二维结构， 是不是就可以大幅减小维度呢？



进而可以想到我们可以借助霍夫曼树来完成这一点。



霍夫曼树是一种基于霍夫曼编码算法生成的特殊的二叉树。我们知道对于一个确定的二叉树，树中的每一个节点到根节点的最短路径是唯一的。我们就可以利用这个路径来生成每一个节点的编码，也就是说每一个节点的编码都是唯一的。



对于树上的每一个节点，向着它左孩子移动的路径编码为0，向右孩子移动的路径编码是1。这样书中每一个节点的路径就可以用一串01二进制来编码。每一个节点的编码都是独一无二的。



![](https://moutsea-blog.oss-cn-hangzhou.aliyuncs.com/image-20240527201638178.png)



![](https://miro.medium.com/v2/resize:fit:972/1*CV76nAMlVgRK0W9NXBlb-A.png)



引入霍夫曼树之后，我们可以保证每一个单词的编码长度不超过$\log |V|$。也就是说对于每一个样本，我们最多只需要更新$\log |V|$个节点即可，从而可以大大减小复杂度。



## 负采样



通过层次`softmax`的方法，我们可以大大降低模型训练时候的参数规模，从而大大提升训练速度。



但有一个问题没有办法解决，就是缺少负样本。



在原本的模型当中，和中心词相关联的$t - 1$个单词组成的单词对可以视作正例，其余的单词对自然就是负例。我们一次更新的时候，会同时更新正负样本。而使用了层次softmax之后，只有正例经过的节点会被更新到，负样本完全被忽略了。



那么有没有办法既能同时更新正负样本，又可以提升模型的收敛速度呢？



还真的有，这个方法非常简单粗暴，就是采样。



原本的词表大小很大，每次更新的时候复杂度很高。但我们不一定要全部都更新一遍参数，其实也没必要，毕竟不是所有的单词对都有意义，有些相同词性的词天生就不可能相连。所以我们是不是不必更新全量，只挑选一部分进行更新呢？



确实可以，并且经过实测效果非常好，甚至比层次softmax更好。



![](https://moutsea-blog.oss-cn-hangzhou.aliyuncs.com/image-20240527202120436.png)



它的实现也很简单，模型结构和原版完全一样，只不过在训练的时候，并不是对所有负样本的节点进行更新，而是会随机选取若干个，只针对这些进行更新。



![](https://moutsea-blog.oss-cn-hangzhou.aliyuncs.com/image-20240527215236172.png)



负采样几乎没有任何劣势，可以看做是一个有益无害的方法。



我们将会在下一篇当中来亲自训练一下`word2vec`的模型，不过现在词向量已经很成熟了，有了封装好的库可以使用，我们不再需要从头开始实现了。



最后，宣传一下我的星球。



程序员副业搞钱计划，目前在更新《穷爸爸与富爸爸》读书笔记，欢迎加入讨论





![](https://moutsea-blog.oss-cn-hangzhou.aliyuncs.com/%E6%98%9F%E7%90%83%E4%BC%98%E6%83%A0%E5%88%B8%20(8).png)