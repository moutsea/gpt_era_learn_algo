大家好，这篇文章我们开始深度学习模型的实战。



在我的公众号`Coder梁`后台回复`ai`，获取无魔法无限使用GPT4.0的方式。





在实战中学习，通过应用进一步加深我们对知识和技能的理解。这个例子来源于深度学习原理与Pytorch实战一书第二版。





数据我已经下载好了，大家可以从我的github里获取：`https://github.com/moutsea/gpt_era_learn_algo`





## 问题背景





我们都知道共享单车非常方便，但共享单车的分布是一个很大的问题。早高峰的时候大量的单车集中在地铁口附近，而到了晚上下班时间，地铁口的单车又会一辆不剩。这就导致了经常会出现需要用车的时候看不到车，不需要用车的时候，车到处都是。





这对于用户以及平台来说都是非常不利的，为了解决这个问题，共享单车公司经常会需要雇佣一些工人来搬运单车。使得单车的分布尽可能合理一些，不会过于集中也不会过于分散。





但由此又诞生了新的问题，单车公司如何知道什么时候应该搬运单车，以及搬运的数量呢？





由于单车的分布呈现一定的规律性，比如和上班时间、节假日等信息息息相关。因此非常适合机器学习模型来进行预测。





## 检查数据





下载好数据之后， 通常我们都会来人工检查一下数据，查看一下字段分布等信息。



首先是数据读取：



```python
import pandas as pd

df = pd.read_csv('./data/bike_predictor/hour.csv')

df.head()
```



![](https://moutsea-blog.oss-cn-hangzhou.aliyuncs.com/image-20240201201939745.png)



这里我们的原始数据是`csv`格式，我们可以使用`pandas`库来进行处理。



`pandas`是一个著名的也是非常常用的Python计算科学库，提供非常丰富的数据处理的api。它最大的特点是可以直接读取各种表格类型的文件来创建`dataFrame`（数据帧），是一种表格类型的数据结构，用来处理结构化数据。



这里我们使用的`api`是`read_csv`它可以从`csv`文件直接读取数据并且创建`dataFrame`。`df.head()`用来展示表格的头部几行数据，我们也可以传入一个整型的参数，用来控制展示的数据行数。



更多的用法直接询问ChatGPT即可。



![](https://moutsea-blog.oss-cn-hangzhou.aliyuncs.com/image-20240201202614752.png)



这里的表格有很多列，光看字段名可能不一定能理解每个字段的含义。



好在原始数据当中提供了`readme`文件，通过查阅`readme`可以看到各个字段的说明。说明是英文的，我们直接丢给ChatGPT翻译即可。



![](https://moutsea-blog.oss-cn-hangzhou.aliyuncs.com/image-20240201203256902.png)



很明显，我们希望模型预测的列是`cnt`，即单车的数量。那么除了`cnt`之外的列就是我们给模型提供的数据，在机器学习领域，这些数据被称为特征。



既然要预测`cnt`，不如我们先来看看`cnt`的数据分布，看看它有什么特点。



我们先选出前10天的数据，然后画出`cnt`随着时间变化的曲线。这需要用到`matplotlib`，如果你不会画，或者有任何问题，都也可以询问ChatGPT。



```python
import matplotlib.pyplot as plt

ndf = df[df['dteday'] < '2011-01-11'].copy()
ndf['datetime'] = pd.to_datetime(ndf['dteday']) + pd.to_timedelta(ndf['hr'], unit='h')

plt.plot(ndf['datetime'], ndf['cnt'])
plt.gcf().autofmt_xdate()
```

![](https://moutsea-blog.oss-cn-hangzhou.aliyuncs.com/image-20240201204827950.png)



## 数据预处理



数据获取好了之后，下一步是不是就是训练模型了？



其实还没有，我们都知道对于一个算法工程师来说，**日常工作当中80%以上的时间都是花在数据预处理和特征的制作上**。我们这里得到的仅仅是原始数据，即使我们不制作额外的特征，也是不能直接拿来训练模型的。



为什么呢？比较大的一个问题是数据的分布和格式。



比如当中的日期这一列，是字符串特征，字符串特征是不能作为神经网络的输入的。我们也很难从日期这样的字段上加工出有效的信息来，既然如此，那么抛弃它就是最好的做法。



`instant`这一列也是要抛弃的，同样因为编号并没有提供额外的信息。



除开这两列之外，其他列也是有问题的。比如`hr`的取值范围是0到23，而`temp`，`atemp`和`hum`列的取值范围都是0到1，`casual`和`registered`的范围更大。不同的列的取值范围不一样对于模型来说是有很大影响的。



我们可以想象一下，在线性回归模型当中，预测的时候本质上是进行的一个加权求和的运算。有些列取值范围比较大，那么对于模型的结果的影响力自然也就越大。而有些列取值范围比较小，那么对结果的影响也就越小。



但问题是，并非取值范围越大的列越有用，如果数据中的噪声过大，会直接拉低模型的效果。



在这篇文章当中我们先不看如何处理这些特征，先来解决一下特征分布不均的问题。



解决的思路很简单，既然每一列分布不一致，那么我们手动做一下处理，把每一列的取值范围弄成一样不就可以了么？



在神经网络当中，我们通常会将特征进行**归一化**，将它们的值域变成0到1之间。



![](https://moutsea-blog.oss-cn-hangzhou.aliyuncs.com/image-20240201211257989.png)



## 实操



首先，我们需要提取出特征和`label`，`label`即模型要预测的列。



前面说了，特征需要去掉`instant`和`dteday`这两列，所以特征的范围是从第三列（下标2）到倒数第二列。`label`即最后一列。



我们可以使用`pandas`中的`iloc`函数来直接切片。将特征的数据放入`X`当中，`label`数据放入`y`当中。这也是机器学习常规的做法。



由于数据是二维的，所以切片也有两个维度。第一个维度是行，我们当然是所有行都要，所以使用`:`，表示所有行都要。逗号分隔维度，对于`X`，我们需要从第三列开始，所以第二个列维度的切片方式是`2:`。



对于`y`来说，我们只需要倒数第一列，传入`-1`即可。



```python
X = df.iloc[:, 2:]
y = df.iloc[:, -1]
```



接下来是归一化的部分，我们并不需要手动实现，`scikit-learn`库中替我们封装好了相关功能，我们直接拿来用即可。



使用的过程非常简单，除去`import`之外只有两行。一行是初始化，一行是执行。



```python
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X = scaler.fit_transform(X)
```



我们输出一行检查下，可以发现所有列的范围都变成0到1了。



![](https://moutsea-blog.oss-cn-hangzhou.aliyuncs.com/image-20240201212245634.png)



相信大家应该都知道，这么操作是有很大问题的。我们没有做任何处理，直接把所有数据都给归一化了。



但我们先忽略不合理的地方，先把整个流程跑通再来慢慢做优化。



## 训练



数据都准备好了，接下来就是模型的开发和训练了。



在我们开发模型之前，我们需要介绍一个新的概念——激活函数（activation function）。



首先，什么是激活函数？



激活函数本质上就是非线性函数，常用的激活函数种类非常有限，基本上就`relu`,`sigmoid`,`tanh`,`softmax`这五种。其他的一些激活函数不少也都是从这几种演化来的。



这几个函数也都很简单，并不复杂，我们一个一个来看。首先是`relu`，它的图像是这样：



![](https://moutsea-blog.oss-cn-hangzhou.aliyuncs.com/relu-new.png)



`sigmoid`函数长这样：



![](https://moutsea-blog.oss-cn-hangzhou.aliyuncs.com/image-20240201214016502.png)



`tanh`和`softmax`表达式也都不复杂，大家感兴趣可以去查一下。



这些函数的式子容易理解，但为什么需要这些函数呢？它们能起到什么作用呢？



很简单，因为这些激活函数本身都不是线性的，所以我们把它们加入神经网络之后可以将神经网络从线性模型变成非线性模型。



我们在实现线性回归的时候谈到过，单层的神经网络本质上就是线性回归，对数据进行的是一个线性变换。在数学上可以证明，若干层线性变换的叠加，可以等价于一个特定的线性变换。也就是说如果不加入激活函数，不论我们有多少层网络，本质上仍然是一个线性模型。



而线性模型是无法表示数据中的非线性部分的，这就是最大的问题。



![](https://moutsea-blog.oss-cn-hangzhou.aliyuncs.com/8EEBFE5901525DC67F2EAF81110_378D40D9_16FDE.png)



`tensorflow`官网有一个演示的demo。这当中的数据集就是非线性的，所谓非线性也就是用线性模型无法表示的数据。比如图中的数据是一个圈，而线性模型会把平面用直线分割成两个部分，显然无论使用什么样的线性模型都无法将图中的两类数据分隔开。



如果我们使用线性的激活函数（等同于不使用），可以发现不论我们迭代多少轮，模型都无法收敛。



网址：`https://playground.tensorflow.org/#activation=relu&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4&seed=0.67223&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false`



![](https://moutsea-blog.oss-cn-hangzhou.aliyuncs.com/image-20240201215020896.png)



而如果我们使用`Relu`激活函数，模型很快就收敛了，从图中也看得出来，模型把数据分类得很好。



![](https://moutsea-blog.oss-cn-hangzhou.aliyuncs.com/image-20240201215110081.png)



大家感兴趣可以亲自去体验一下，感受一下激活函数的作用。



理解了激活函数的概念之后，我们就可以来实现模型了。



```python
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F

class Net(nn.Module):
    def __init__(self, input_dim, hidden_dim):
        super(Net, self).__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, 1)
        self.relu = nn.ReLU()

    def forward(self, x):
        x = self.relu(self.fc1(x))
        return self.fc2(x)
```



之前我们实现的模型借助了`nn.Sequential`函数，这里我们使用另外一种方式，也是我个人比较喜欢的方式，就是通过Python中的类继承`nn.Module`来实现，一个模型就是一个类，非常直观，非常pythonic。



在`Net`当中我们创建了两层神经网络，由于`fc1`的输出是`fc2`的输入，所以我们需要保证它们的`dim`相等。激活函数并不会改变中间结果的`shape`，每一层神经网络之间只需要满足矩阵乘法的要求即可。



我们在`__init__`函数当中定义网络的结构，`super(Net, self).__init__()`是固定的写法，初始化父类的属性和方法。



我们在`forward`函数当中实现网络的运算功能，即从输入到输出的传播过程，这个过程被称为**前向传播**。与之相反的，训练过程中梯度的传递称为**反向传播**。



唯一需要注意的就是我们在`fc1`的结果上加上了激活函数`Relu`。



接下来的流程大家如果看过之前的文章的话应该很熟悉了，基本上就是照搬的训练模板：



```python
from torch.utils.data import DataLoader, TensorDataset

net = Net(15, 8)

X_tensor = torch.tensor(X).float()
y_tensor = torch.tensor(y).float()
dataset = TensorDataset(X_tensor, y_tensor)
data_iter = DataLoader(dataset=dataset, batch_size=32, shuffle=True)

loss = nn.MSELoss()
optim = torch.optim.SGD(net.parameters(), lr=0.0003)

num_epochs = 3

losses = []
for epoch in range(num_epochs):
    for dtx, dty in data_iter:
        l = loss(net(dtx).squeeze() ,dty) # 注意这里的squeeze
        optim.zero_grad()
        l.backward()
        optim.step()
        with torch.no_grad():
            l = loss(net(X_tensor).squeeze(), y_tensor) # 注意这里的squeeze
            losses.append(l.detach().numpy())
```



需要注意一下，我们在调用`net`模型预测结果之后，调用了`squeeze`。



这是因为模型的预测结果的`shape`是`[B, 1]`，这里的`B`指的是`batch_size`。而`dty`的`shape`是`[B]`，它们的`shape`并不匹配。所以我们需要调用一下`squeeze`，将模型的输出的维度也变成`[B]`。



最后，我们来看一下`loss`的曲线：



![](https://moutsea-blog.oss-cn-hangzhou.aliyuncs.com/image-20240201222314136.png)



虽然我们没做任何处理，但看起来效果还行。



这篇文章肝了好几个小时才写完，如果觉得老梁写得不错的话，欢迎转发扩散。



最后，宣传一下我的星球。



除了Ai什么都有，个人学习成长、理财投资、量化交易心得，持续更新中



![](https://moutsea-blog.oss-cn-hangzhou.aliyuncs.com/%E6%98%9F%E7%90%83%E4%BC%98%E6%83%A0%E5%88%B8%20(6).jpeg)