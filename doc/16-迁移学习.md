大家好，今天我们来聊迁移学习。



公众号`Coder梁`后台回复`ai`，无魔法无限使用GPT4，我个人用了好几个月了，真诚推荐。



## 概念



随着大模型的兴起，迁移学习也成了一个非常热门的研究领域。



我们先来看下GPT对于迁移学习概念的介绍：



![](https://moutsea-blog.oss-cn-hangzhou.aliyuncs.com/image-20240416220516462.png)



如果觉得看起来吃力的话，我帮大家举一个很简单的例子。



其实我们人类也经常在使用迁移学习，我们的很多技能的底层逻辑都是相通的。比如一个会骑自行车的人，大概率也能很快学会骑摩托车或小电驴。



自行车和摩托车在骑行上肯定有很多差异，原理上也大相径庭。一个是依靠人力，一个是依靠内燃机，一个速度慢，一个速度极快……但是它们都对保持平衡有较高的要求，当你能驾驭自行车的平衡之后，自然也能驾驭摩托车的。



在深度学习中的迁移学习也是类似的，一些任务对于模型底层能力的要求也非常相似。比如语言模型识别、理解字词，消除歧义的能力，图片模型识别图像、提取图片特征的能力。模型对于能力的体现都依托于参数，通过对于模型参数的迁移和拷贝，我们能复刻大部分模型的底层能力，这也是迁移学习的基础。



## 例子



我们来看`pytorch`官网中蜜蜂和蚂蚁的例子。



我们有一些蜜蜂和蚂蚁的图片，我们希望模型能够学会分辨这两种昆虫。数据集下载链接: `https://download.pytorch.org/tutorial/hymenoptera_data.zip`



大概是这样的一些图片：



![](https://moutsea-blog.oss-cn-hangzhou.aliyuncs.com/image-20240416231534548.png)



很明显，这是一个二分类问题。蜜蜂和蚂蚁是不同的类别，我们可以使用卷积神经网络来训练模型。只需要调整一下模型的参数和结构，其他部分变化不大，甚至完全不变都可以，只需要将`classes`设置成2即可。



但如果你真这么去做了，就会发现模型的效果很差，准确率只能达到50%左右，和瞎猜相比都没什么优势。



会有这样的情况也很简单，因为我们的**样本太少了**。一共只有244个样本，对于比较复杂的图像数据，只有这么少的样本，显然是无法很好地拟合的，这是数据层面天然的劣势。



这也是我们需要引入和使用迁移学习的原因。



怎么引入呢？其实很简单，也很直观，就是引入一些已经在图片识别问题当中被训练好并且取得不错结果的模型。



通常卷积神经网络中前面卷积层的部分用来识别图中的特征，而卷积层之后的全连接层则主要用来对特征进行识别。因此我们在加载模型的时候，只需要加载其中卷积层的部分即可。载入了卷积层的参数之后，相当于迁移了已经训练好模型的图片特征提取和识别的能力，在此基础上，可以非常明显地帮助我们提升模型的效果。



## ResNet



如今开源模型非常普遍，尤其在计算机视觉和NLP领域，可选的开源模型非常多，我们可以非常方便地找到开源模型。



甚至一些比较经典的模型直接被集成进了深度学习框架当中，今天我们要使用的就是计算机视觉领域一个非常经典的模型——ResNet，它在2015年由微软亚洲研究院提出，解决了多层卷积网络叠加导致梯度消失的问题。



![](https://moutsea-blog.oss-cn-hangzhou.aliyuncs.com/image-20240417210650494.png)



对照解释一下下图，图中上游的输出被认为是$F(x)$，而我们这一层的学习目标是$H(x)$。我们这一层不再是直接学习$F(x)$到$H(x)$的映射，而是学习$H(x) - x$。因为$x$的值会从上层直接绕过本层传递到输出结果当中，因此这一层的学习目标也就随之发生了改变。



这么做看起来有些多余，但实际上通过这样的变形，它可以很好地保证梯度可以层层传递，不会因为网络层数太多而导致梯度消失。



![](https://moutsea-blog.oss-cn-hangzhou.aliyuncs.com/image-20240417210609998.png)



`ResNet`开源模型根据层数不同有多个版本，由于在当前问题当中，图片的规模比较小，因此我们选择规模最小的18层网络的`ResNet-18`即可。



## 代码实现



首先是`import`部分，我们在这里引入必要的库，无需过多赘述。



```python
import torch
import torch.nn as nn
import torch.optim as opt
import torch.nn.functional as F

from torchvision import datasets, transforms, models, utils

import matplotlib.pyplot as plt
import numpy as np
import time
import copy
import os
```



接着是数据处理的部分：



```python
data_dir = 'data/hymenoptera_data'
image_size = 224

data_transforms = {
    'train': transforms.Compose([
        transforms.RandomResizedCrop(image_size),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
    'val': transforms.Compose([
        transforms.Resize(256),
        transforms.CenterCrop(image_size),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
}

image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),
                                          data_transforms[x])
                  for x in ['train', 'val']}
dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,
                                             shuffle=True, num_workers=4)
              for x in ['train', 'val']}
dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}
class_names = image_datasets['train'].classes

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
```



这里的数据预处理部分是我们第一次遇到，有许多细节值得细说，我们一个一个来看。



由于模型的输入是一张图片，这次不同于上次的手写数字，这次的图片是拥有完整3个channel的图片。图片信息较为复杂，因此对它的处理通常也会涉及多个步骤。这里我们使用`torchvison.transforms`库来组装多个预处理步骤来处理图片样本。



仔细看会发现训练和验证集的预处理逻辑是不同的，在训练时我们使用的是`RandomResizedCrop`，这用来对图片进行随机裁剪，`RandomHorizontalFlip`这一步是随机对图片进行水平翻转。这两个步骤都是为了**增加图片的随机性**，让模型拿不到最规整的图片，从而迫使它提升从碎片信息中进行学习的能力，即提升模型的**健壮性**。



而在验证的时候，我们更看重**一致性和公平性**，所以我们不再进行随机裁剪，而使用`Resize`再取中间的方法，来保证每次获取到的都是图片的核心区域，这样模型在验证集的表现才是稳定和公平的。



训练集和验证集的最后一步都是`Normalize`，这是用来对图片进行标准化的。它后面输入的参数是图片在3个channel上的均值和标准差。



最后，我们获取了`device`，即我们电脑上用来训练的设备。如果你有英伟达的显卡，并且正确安装了cuda的话，那么得到的结果应该是这样的：



![](C:\Users\moutsea\AppData\Roaming\Typora\typora-user-images\image-20240417215625597.png)



如果没有也没关系，CPU一样能进行模型的训练和推理，只不过可能速度稍稍慢一些而已。



我们还可以开发出一些工具函数，比如用来拼接和展示图片，我们可以更直观地看到模型的输入结果。



```python
def imshow(inp, title=None):
    # 还原标准化之前的结果
    inp = inp.numpy().transpose((1, 2, 0))
    mean = np.array([0.485, 0.456, 0.406])
    std = np.array([0.229, 0.224, 0.225])
    inp = std * inp + mean
    inp = np.clip(inp, 0, 1)
    plt.imshow(inp)
    if title is not None:
        plt.title(title)
    plt.pause(0.001) 


inputs, classes = next(iter(dataloaders['train']))

# 将一个batch的图片拼接在一起
out = utils.make_grid(inputs)

imshow(out, title=[class_names[x] for x in classes])
```



![](https://moutsea-blog.oss-cn-hangzhou.aliyuncs.com/image-20240417220030769.png)



这些都准备就绪之后，我们就可以开始准备模型的部分了。



由于我们这里要做的是迁移学习，即加载一个已经训练好的模型的部分参数，因此模型准备的部分会和之前我们从头开始开发模型的情况有一些区别：



```python
model = models.resnet18(weights='IMAGENET1K_V1')
# 替换全连接层
num_ftrs = model.fc.in_features
model.fc = nn.Linear(num_ftrs, 2)
criterion = nn.CrossEntropyLoss()
optimizer = opt.Adam(model.parameters(), lr=0.0001)
```



这里我们首先载入了`resnet18`的模型参数，如果是第一次运行的话，会触发网络下载。因为相关的模型参数较大(44MB)，并不是存在本地的。



原本的`resnet18`模型是在一个拥有1000个类别的大型数据集上训练的，因此它最后的输出层是一个1000维的`softmax`。而在我们的问题场景当中，只需要进行二分类。所以我们需要手动替换这个部分，这就相当于我们保留了模型最后全连接层之前的部分，包括网络结构和参数，仅仅替换了最后的全连接层。



这些都搞定了之后，就是我们喜闻乐见的模型训练部分了。



```python
best_model_params_path = os.path.join('../resource/resnet', '6-resnet.pt')
model.to(device)
since = time.time()

torch.save(model.state_dict(), best_model_params_path)
best_acc = 0.0
num_epochs = 20

for epoch in range(num_epochs):
    print(f'Epoch {epoch}/{num_epochs - 1}')
    print('-' * 10)

    for phase in ['train', 'val']:
        if phase == 'train':
            model.train()  # Set model to training mode
        else:
            model.eval()   # Set model to evaluate mode

        running_loss = 0.0
        running_corrects = 0

        for inputs, labels in dataloaders[phase]:
            inputs = inputs.to(device)
            labels = labels.to(device)

            optimizer.zero_grad()

            with torch.set_grad_enabled(phase == 'train'):
                outputs = model(inputs)
                _, preds = torch.max(outputs, 1)
                loss = criterion(outputs, labels)

                if phase == 'train':
                    loss.backward()
                    optimizer.step()

            # statistics
            running_loss += loss.item() * inputs.size(0)
            running_corrects += torch.sum(preds == labels.data)

        epoch_loss = running_loss / dataset_sizes[phase]
        epoch_acc = running_corrects.double() / dataset_sizes[phase]

        print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')

        if phase == 'val' and epoch_acc > best_acc:
            best_acc = epoch_acc
            torch.save(model.state_dict(), best_model_params_path)

    print()

time_elapsed = time.time() - since
print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')
print(f'Best val Acc: {best_acc:4f}')
```



![](https://moutsea-blog.oss-cn-hangzhou.aliyuncs.com/image-20240417222249195.png)



这里我们一共进行了20轮的训练，就可以得到最高94%以上的准确率，可以说是非常高了。对比我们从头训练的50%左右的准确率，显然是质的飞跃。



如果没有GPU单纯使用CPU进行训练的话，大概需要20分钟左右。而使用GPU，基本上能在2分钟之内跑完，训练速度差了大概10倍左右，可以说是非常可观了。



借助于开源的模型参数，我们很容易取得了非常不错的效果，甚至不需要太多调整就可以用来实际应用。这对于一些没有能力搜集大量数据集进行大规模训练的小型企业或机构而言，是巨大的福音。尤其是现在大模型兴起以后，利用开源模型的参数，在现实问题场景上进行微调的做法几乎是标配，所以近几年迁移学习也因此变得越来越流行。



最后，宣传一下我的星球。



除了Ai什么都有，目前在更新《穷爸爸与富爸爸》读书笔记，欢迎加入讨论



![](https://moutsea-blog.oss-cn-hangzhou.aliyuncs.com/image-20240404143631289.png)