大家好，这是词向量的最后一篇，我们来亲自训练一个`Word2Vec`模型。



公众号`Coder梁`后台回复`ai`，无魔法无限使用GPT4，我个人用了好几个月了，真诚推荐。



## 前期准备



我在之前的文章中说过，现在`word2vec`已经是很成熟的技术了，有许多成熟的封装好的解决方案，我们可以直接拿过来用，而不再需要从头开始实现了。



不仅如此，即使是在技术要求更高的大公司中，这些成熟的底层工具往往也不是从头开始研发的。所以大家不必有心理负担，对于现成的组件先熟练使用再思从零实现。



这里我们需要用到一个新的库，叫做`gensim`。



这个库在NLP领域非常有名，封装了许多NLP的基础功能，比如词向量以及TF-IDF等。



![](https://moutsea-blog.oss-cn-hangzhou.aliyuncs.com/image-20240603203641266.png)



和之前一样，我们使用`pip`来进行安装：



```bash
pip install gensim
```



## 预处理



`gensim`虽然封装了词向量的训练过程，但是前期的数据处理和分词还是需要我们来完成。



这部分逻辑和之前NPLM的完全一样，所以就不过多赘述了。



唯一的一个区别是在之前NPLM中我们是把单词加工成了`trigram`的形式，这里我们是按行处理，尽可能保留了语句的连贯性。



![](https://moutsea-blog.oss-cn-hangzhou.aliyuncs.com/image-20240603204601131.png)



```python
import jieba
import string

punctuation = set(string.punctuation)
# 添加中文常见标点
punctuation.update(['，', '。', '！', '？', '、', '；', '：', '“', '”', '‘', '’', '（', '）', '《', '》', '……', '—', '\u3000'])

def clean_text(text):
    # 使用 jieba 进行分词
    words = jieba.lcut(text)
    # 过滤掉标点符号和空白字符
    filtered_words = [word for word in words if word not in punctuation and not word.isspace()]
    return filtered_words

lines = []
with open('./data/nplm/xiyouji.txt', 'r', encoding='utf-16') as f:
    for line in f:
        words = clean_text(line)
        if len(words) > 0:
            lines.append(words)
```



## 使用



`gensim`的使用非常简单，只有一行：



```python
model = Word2Vec(lines, vector_size=20, window=2, min_count=0)
```



整个过程非常快速，只跑了1s不到。这也是`Word2vec`技术的最大突破，极大增加了模型收敛的速度。之前我们训练NPLM的时候，足足用了好几个小时。而现在一秒钟就收敛了……



![](https://moutsea-blog.oss-cn-hangzhou.aliyuncs.com/image-20240603204723586.png)



近义词的结果看起来还不错。



![](https://moutsea-blog.oss-cn-hangzhou.aliyuncs.com/image-20240603205509275.png)



我们用同样的方法获取词向量，然后降维绘制分布图。



```python
# 获取词向量和id
word_vectors = model.wv
words = list(word_vectors.key_to_index)
vectors = [word_vectors[word] for word in words]
```



```python
from sklearn.decomposition import PCA
# 使用PCA算法对向量进行降维
X_reduced = PCA(n_components=2).fit_transform(vectors)
```



```python
import matplotlib.pyplot as plt
import matplotlib
import os

fig = plt.figure(figsize = (30, 20))
ax = fig.gca()
ax.set_facecolor('white')
ax.plot(X_reduced[:, 0], X_reduced[:, 1], '.', markersize = 1, alpha = 0.4, color = 'black')

highlight_words = ['悟空', '八戒', '悟净', '观音菩萨', '如来', '罗汉', '玉帝', '大圣','唐三藏', '和尚', '道人', '星君', '青牛', '王母', '神通', '星宿', '菩萨', '妖怪', '嫦娥', '天蓬', '卷帘']

zhfont1 = matplotlib.font_manager.FontProperties(fname='./data/nplm/HuaWenFangSong-1.ttf', size=22)

for w in highlight_words:
    if w in words:
        ind = word_to_idx[w]
        xy = X_reduced[ind]
        plt.plot(xy[0], xy[1], '.', alpha =1, color = 'red')
        plt.text(xy[0], xy[1], w, fontproperties = zhfont1, alpha = 1, color = 'black')
        
plt.show()
```



这个降维之后的结果和之前NPLM的完全不同。



大量的向量聚集在了左侧，而剩余的大部分空间则非常稀疏。不过这个只能当做参考，毕竟向量降维之后会损失大量的信息。



![](https://moutsea-blog.oss-cn-hangzhou.aliyuncs.com/image-20240603205844575.png)



如果我们换一种降维的算法，又会得到完全不同的结果：



```python
from sklearn.manifold import TSNE

tsne = TSNE(n_components=2, random_state=23)
X_reduced = tsne.fit_transform(vectors)
```



![](https://moutsea-blog.oss-cn-hangzhou.aliyuncs.com/image-20240603210603506.png)



同样的向量，不同的降维方法，结果完全不同。



但仔细观察，可以发现虽然这两幅图分布完全不同，但是有一些词语的组合是非常接近的，比如<嫦娥、星宿>，<天蓬，卷帘，悟净，王母>。



所以真要衡量词向量的结果，最好的方式还是通过近义词。



最后，宣传一下我的星球。



程序员副业搞钱计划，目前在更新《穷爸爸与富爸爸》读书笔记，欢迎加入讨论



![](https://moutsea-blog.oss-cn-hangzhou.aliyuncs.com/%E6%98%9F%E7%90%83%E4%BC%98%E6%83%A0%E5%88%B8%20(8).png)