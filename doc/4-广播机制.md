今天这篇文章和大家来聊一个非常重要的概念——广播。



如果是纯小白的话，看到这个概念的时候应该非常奇怪，可能很难把这个词和深度学习联系起来。在科学计算领域，广播表示的含义其实是两个矩阵之间维度的“对齐”。



我们来举个简单的例子，假设我们现在有一个`tensor` `x`，它的维度很大，一共包含了一亿个元素。假设我们有一个需求要对这个`tensor`做一个简单的加法操作，将它当中每个元素加上10。我们应该怎么操作？



由于`tensor`支持整体操作，一种做法是通过`torch.ones`创建一个同样`shape`的`tensor`，然后将它和`x`相加。



但其实并不需要这么麻烦，我们只需要用`x += 10`即可。



![](https://moutsea-blog.oss-cn-hangzhou.aliyuncs.com/image-20240124225304218.png)



我们来详细看下内部的逻辑，首先，Pytorch会把这里的10看成是一个标量，也就是一个`shape`为空的量。也就是说在执行的时候，其实还是两个`tensor`相加，只不过这两个`tensor`的`shape`不同。



我们说的广播其实就是一个处理两个`shape`不同时`tensor`的机制。



也就是说在`tensor`的运算当中，两个`tensor`的`shape`相同并不是必要条件，不同不一定不能算，广播就是专门解决这个问题的。



我们来看下ChatGPT给出的定义，它说了很多，重点是红框内的内容。



![](https://moutsea-blog.oss-cn-hangzhou.aliyuncs.com/image-20240124230328281.png)

我们一条一条来看，首先是维度对齐。



这里的对齐就是字面意思，比如两个`tensor`其中一个的`shape`是`(3, 4, 2, 5)`，另外一个的`shape`是`(3, 5)`。从末尾对齐之后大致是这样：



```python
(3, 4, 2, 5)
(1, 1, 3, 5)
```



我们注意到第二个`tensor`本来只有两维，对齐之后也变成了4维，空缺的部分使用了1来填充。



执行了维度对齐之后，下一个步骤是维度拓展。维度拓展也很好理解，其实就是复制。会把`tensor`中维度为1的那一维度进行复制，复制成另外一个`tensor`对应的维度。



比如在刚才的例子当中，第二个`tensor`拓展之后的`shape`是`[1, 1, 3, 5]`另外一个`tensor`的维度是`[3, 4, 2, 5]`。这时候就会将`tensor`的前两维通过复制的方式进行拓展。拓展之后，第二个`tensor`的维度会变成`[3, 4, 3, 5]`。



我们再来看一个例子，我们有这么两个`tensor`：



```python
a = torch.arange(3).reshape((3, 1))
b = torch.arange(2).reshape((1, 2))
```



这里的`torch.arange()`用来生成一个一维的连续数字的序列，比如`torch.arange(3)`得到的结果就是`[0, 1, 2]`，那么`torch.arange(2)`得到的自然就是`[0, 1]`了。



`reshape`操作之前已经介绍过了，所以`a`和`b`如下：



![](https://moutsea-blog.oss-cn-hangzhou.aliyuncs.com/image-20240125082932089.png)



如果我们把`a`和`b`相加会得到什么呢？



我们照着刚才说的步骤执行一次就明白了，首先我们列出它们的`shape`。`a`的`shape`是`[3, 1]`，而`b`的`shape`是`[1, 2]`。秩都是2，不需要额外对齐了。接着我们进行拓展，即是把两个`tensor`当中维度为1，而对方不为1的进行拓展。拓展之后`a`和`b`的`shape`都是`[3, 2]`。



我们可以理解成将`a`在第1个轴（下标从0开始）拷贝将维度变成了2，而`b`则是在第0轴拷贝，将维度变成了3。也可以理解成将两个`a`在第1轴，3个`b`在第0轴拼接在了一起。Pytorch当中我们可以使用`torch.cat()`方法来对若干个`tensor`进行拼接。通过参数`dim`指定在哪一维进行拼接。



![](https://moutsea-blog.oss-cn-hangzhou.aliyuncs.com/image-20240125083634731.png)



我们再来看看`a + b`的结果：



![](https://moutsea-blog.oss-cn-hangzhou.aliyuncs.com/image-20240125084140178.png)



是不是就是上面两个拼接之后`tensor`的和，这种自动对齐维度并进行拷贝来使得`tensor`符合运算的`shape`要求的过程就叫做广播。



然而广播并不是万能的，但遇到通过对齐和拓展解决不了的情况时，就会报错。



比如下面这个例子，`x`的`shape`是`[3, 4, 2, 5]`，而`y`的`shape`是`[3, 5]`。虽然`x`的维度当中也包含了`3, 5`。但由于对齐是末尾对齐的，所以对齐和拓展之后，`y`的`shape`会变成`[3, 4, 3, 5]`。第2轴和`x`对不上，此时就会引发报错。



![](https://moutsea-blog.oss-cn-hangzhou.aliyuncs.com/image-20240125090241212.png)



**切记广播进行`shape`对齐时，是末尾对齐。**



如果我们把`y`的`shape`变更成`(2, 5)`，那么就不会报错了。

![](https://moutsea-blog.oss-cn-hangzhou.aliyuncs.com/image-20240125090412706.png)



广播这个概念非常重要，因为在实现模型的时候，这个机制非常常用。如果搞不懂的话，就会给自己阅读和编写代码增加很多障碍。



欢迎加入我的星球~



![](https://moutsea-blog.oss-cn-hangzhou.aliyuncs.com/%E6%98%9F%E7%90%83%E4%BC%98%E6%83%A0%E5%88%B8%20(6).jpeg)