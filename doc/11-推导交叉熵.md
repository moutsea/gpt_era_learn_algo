大家好，我们这篇文章来推导一下交叉熵。



公众号`Coder梁`后台回复`ai`，获取无魔法无限使用GPT4.0的方式。



## 推导损失函数



关于交叉熵我们有两种基于不同理论的推导方式，但最终却会得到完全一样的结果，不得不说这非常神奇。希望大家感兴趣的话，都能仔细看完推导，相信一定会有收获的。



我们先来确定一下符号，对于一条样本来说，我们用$x$表示，这条样本的标签记作$y$。全体样本我们记作$X$，全体的标签我们记作$Y$。模型对于单条样本的预测结果记作$\hat{y}$，模型对所有样本的预测结果是$\hat{Y}$。



这里我们仍然是二分类问题，那么对于一条样本来说，它的标签只会有两个取值0和1。我们知道$\hat{y}$由于`sigmoid`激活函数的缘故，它的取值范围是$(0, 1)$。那么，我们自然希望当样本标签是0时，$\hat{y}$越小越好，而当标签是1时，$\hat{y}$越大越好。



这里做了分情况讨论，会使得出现多个式子，比较复杂。好在只有两种情况，我们可以用一个特殊的式子来兼容这两种情况：


$$
P(y|x) = \hat{y}^y(1-\hat{y})^{(1-y)}
$$


我们可以代入一下，如果$y=0$，那么剩余的式子是$P(y|x) = 1 - \hat{y}$。如果$y=1$，剩余的式子是$P(y|x) = \hat{y}$。不论是哪一种情况，我们都希望$P(y|x)$越大越好，它表示的都是模型预测正确的概率。



我们希望它越大越好，其实等价于它的相反数越小越好。这完美符合我们对损失函数的诉求，不过这个式子还是有些复杂，因为它带有乘方。解决这个问题也很简单，由于$P(y|x)$恒大于0，我们对它求对数即可。



我们令$J=-\log P(y|x)$，我们代入可以得到：


$$
J = -\log P(y|x) = -(y \log \hat{y} + (1-y) \log (1 - \hat{y}))
$$


这个式子再除以一个样本的数量，就是损失函数了。



这个损失函数被称作是交叉熵，为什么会有这个名称呢，它是怎么来的呢？这就需要从另外一个角度来推导了。



## 信息论



熵这个概念应用非常广泛，比较经典的一个应用是在热力学当中，用来反应一个系统的混乱程度。根据热力学第二定律，一个孤立系统的熵不会减少。比如一盒乒乓球，如果把盒子掀翻了，乒乓球散出来，它的熵增加了。如果要将熵减小，那么必须要对这个系统做功，也就是说需要有外力来将散落的乒乓球放回盒子里，否则乒乓球的分布只会越来越散乱。



开创了信息论的香农大佬灵光一闪，既然自然界微观宏观的问题都有熵，那么信息应该也有。于是他开创性地将熵这个概念引入信息论领域，和热力学的概念类似，信息论当中的熵指的是信息量的混乱程度，也可以理解成信息量的大小。



### 信息量



举个简单的例子，以下两个句子，哪一个句子的信息量更大呢？



1. 我今天没中彩票
2. 我今天中彩票了



从文本上来看，这两句话的字数一致，描述的事件也基本一致，但是显然第二句话的信息量要比第一句大得多，原因也很简单，因为中彩票的概率要比不中彩票低得多。



相信大家也都明白了，一个信息传递的事件发生的概率越低，它的信息量越大。我们用对数函数来量化一个事件的信息量：


$$
I(X) = -\log (P(X))
$$
因为一个事件发生的概率取值范围在0到1之间，所以log(p(X))的范围是负无穷到0。加上负号之后，$P(X)$越小，$I(X)$越大。即事件发生的概率越小，它带有的信息量也就越大。



### 信息熵



我们上面的公式定义的是信息量，但是这里有一个问题，我们定义的只是事件X的一种结果的信息量。对于一个事件来说，它可能的结果可能不止一种。我们希望定义整个事件的信息量，其实也很好办，我们算下整个事件信息量的期望即可，这个期望就是信息熵。



公式为：


$$
E(X) = \sum P(X)I(X)
$$
我们套入信息量的公式之后，得到的结果就是信息熵：


$$
H(X) = -\sum_{n=1}^n P(x_i)log(P(x_i))
$$

### 相对熵（KL散度）



我们回顾一下信息熵的公式，看起来可能有一点点复杂，但它实际表示的意义就是若干个事件信息量的期望。



如果我们把一个事件看成是一个样本的话，那么$H(X)$表示的就是一组样本的信息熵。我们分别代入样本真实的标签和模型的预测结果，就能得到两个信息熵。



既然有两个信息熵，那么很容易想到我们可以把这两个信息熵作差来表示它们之间的距离，距离越小，说明模型学习的结果越好和真实结果越接近。



没错，计算学家们也是这么想的，他们利索地列出了公式。



真实样本的信息熵为：
$$
H_P(X) = -\sum_{x=1}^nP(x_i)\log (P(x_i))
$$


模型预测的信息熵为，我们使用$Q(x_i)$表示模型给出的样本$x_i$的概率。


$$
H_Q(X)=-\sum_{x=1}^n P(x_i)\log (Q(x_i))
$$


我们把它们两者相减：


$$
\begin{align}
H_Q(X) - H_P(X) &= \sum_{x=1}^n P(x_i) (\log (P(x_i)) - \log (Q(x_i))) \\
&=  \sum_{x=1}^n P(x_i) \log \frac{P(x_i)} {Q(x_i)}
\end{align}
$$


数学家们给这个式子起了一个专有的名字叫做KL散度，专门用来衡量两个概率分布之间的近似程度，记作$D_{KL}(P||Q)$。



这个式子当然可以作为损失函数，但是它还是太复杂了，我们需要进行化简之后才能使用。怎么化简呢？首先，需要明确这个函数的几个特性。



比如这个函数是一个凸函数，怎么证明呢？



很简单，前面说了，它是$D_{KL}(P||Q) = H_Q(X) - H_P(X)$，对于一个确定的事件来说它的概率分布是确定的。也就是说$H_P(X)$是一个常数，既然如此，那么仅剩的变量就是$H_Q(X)$了，也就是$Q$分布的信息熵，它的表达式中只有$log$函数组成，而$log$函数是凸函数，那么$H_Q(X)$也是一个凸函数，进而得到整个KL散度也是凸函数。



这里的证明不是很严谨，大家理解精髓即可。



接下来我们要证明它是恒大于等于0的，怎么证明呢，需要用到Jensen不等式，Jensen不等式仅作用于凸函数，这也就是为什么我们要证明它是凸函数的原因。



Jensen不等式很简单，即$f(E(x)) >= E(f(x))$，即自变量期望处的函数值大于函数值的期望。



![](https://moutsea-blog.oss-cn-hangzhou.aliyuncs.com/4141733945af7264a9241c503af05be5.png)

我们利用这个不等式来进行证明：


$$
\begin{align}
D_{KL} =  \sum_{x=1}^n P(x_i) \log \frac{P(x_i)} {Q(x_i)} =E(\log \frac{P(x_i)} {Q(x_i)}) = E(- \log \frac{Q(x_i)} {P(x_i)})
\end{align}
$$


然后我们利用不等式：


$$
\begin{align}
E(- \log \frac{Q(x_i)} {P(x_i)}) &\geq -\log (E(\frac{Q(x_i)} {P(x_i)}) \\
&= -\log (\sum_{i=1}^n P(x_i)  \frac {Q(x_i)}{P(x_i)}) \\
&= -\log (\sum_{i=1}^n Q(x_i)) \\
&= 0
\end{align}
$$


所以我们证明了KL散度是一个非负值。



### 交叉熵



前文中我们证明KL散度是一个凸函数的时候，其实已经给出重点了。



我们说过对于一个已经确定的样本来说，事件P的信息熵是确定的是一个常数，所以我们可以忽略不计。那么，当我们忽略常数之后剩下的就可以用来作为损失函数了。



去掉事件P的信息熵的部分之后，KL散度公式当中剩下的是这个部分：


$$
\sum_{x=1}^n P(x_i) \log Q(x_i)
$$
我们把这个部分记作是$C(P, Q)$，对于二分类问题来说：
$$
C(P, Q) = -P(x=0)\log (Q(x=0)) - P(x=1)\log (Q(x=1))
$$


由于$P(x=0) +P(x=1) =1. Q(x = 0) + Q(x=1) = 1$，我们令$y= P(x=1), \hat{y}=Q(x=1)$，代入上式可以得到：


$$
C(P, Q) = -(1-y) \log (1-\hat{y}) - y \log (\hat{y}) = -(y \log \hat{y} + (1-y)\log (1 - \hat{y}))
$$


这个式子是不是和我们之前推导得到的交叉熵的公式一模一样？两种完全不同的推导思路殊途同归了。



到这里关于交叉熵就介绍完了，如果大家能坚持看完的话，相信对于它的理解就算是非常深入了。



最后，宣传一下我的星球。



除了Ai什么都有，目前在更新《穷爸爸与富爸爸》读书笔记，欢迎加入讨论



![](https://moutsea-blog.oss-cn-hangzhou.aliyuncs.com/%E6%98%9F%E7%90%83%E4%BC%98%E6%83%A0%E5%88%B8%20(6).jpeg)