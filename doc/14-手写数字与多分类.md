大家好，我们今天来讨论一下多分类的问题。



公众号`Coder梁`后台回复`ai`，无魔法无限使用GPT4。



## 多分类



我们之前详细介绍过二分类模型，还硬核地推导过二分类问题当中损失函数的由来，今天我们来聊聊多分类的问题。



所谓多分类，也就是预测类别数量超过2的分类问题。这类问题在生活当中也非常常见，比如车牌识别、人脸识别、物体检测等，那么它是如何实现的呢？



其实也很简单，就像是二分类问题和回归问题一样，多分类问题的模型主体部分和二分类也是一样的，唯一不同的只有输出层。我们比较容易想到，我们可以让**输出层的神经元数量等于类别的数量，这样，这些神经元中输出结果值最大的，即是模型的预测类别**。



![](https://moutsea-blog.oss-cn-hangzhou.aliyuncs.com/image-20240407201233014.png)



这种想法虽然朴素，但已经很接近了，唯一需要解决的就是归一化的问题。在不加任何激活函数的情况下，模型的输出值的范围可能很大。这就导致了在训练过程当中，计算得到的梯度范围很大，很难稳定收敛。



为了解决这个问题，我们给最后一层输出层加上激活函数`softmax`。它可以完成输出结果的概率化，这样每个神经元的输出结果等同于模型预测样本属于该类别的概率。所有神经元的结果之和为1，即所有类别的概率和为1。


`softmax`的公式非常简单，大家可以参考下下图，或者直接询问GPT。



![](https://moutsea-blog.oss-cn-hangzhou.aliyuncs.com/image-20240407201546860.png)



加入了激活函数之后，模型的输出结果可以视作为样本`i`的概率分布。既然是概率分布，那么我们就可以使用交叉熵来反应它和真实分布之间的差异，这个差异就可以用来作为模型训练时的损失函数。



![](https://moutsea-blog.oss-cn-hangzhou.aliyuncs.com/image-20240407201951381.png)



关于这个部分如果看了有疑惑，可以往前翻一下推导交叉熵损失函数的那篇文章。



总之，在代码实现上，多分类的模型和二分类几乎完全一样，唯一不同的是模型最后一层的神经元数量是`m`个（`m`为类别数量）而非1个，并且激活函数用的是`softmax`而非`sigmoid`。



这时候，我们再来对照一下二分类时`sigmoid`和损失函数的公式，也会发现两者其实没有区别。二分类只不过是多分类问题的一种特殊情况而已。



## 手写数字



为了帮助大家更好地理解和掌握，我们来看一个非常经典的案例——手写数字。



这个案例在几乎每一本机器学习相关课程和书籍中都出现过，堪称经典之最。所以作为初学者，我们自然也不能错过。



![](https://moutsea-blog.oss-cn-hangzhou.aliyuncs.com/image-20240407202814901.png)



`mnist`数据集来源于美国国家标准与技术研究院的原始真实数据集，都是由真人书写搜集的。我找来了一批给大家感受一下，可以看到有一些数字的书写习惯和风格和我们还是有一些差异的。



![](https://moutsea-blog.oss-cn-hangzhou.aliyuncs.com/MNIST-handwritten-digits-dataset-visualized-by-Activeloop.webp)



由于这份数据过于经典且被广泛使用，因此也被集成进了多种数据集合当中。包括`tensorflow`和`pytorch`框架。因此我们安装了`pytorch`之后并不需要额外下载就可以使用它们。



### 数据准备



```python
import torch
import torch.nn as nn
import torch.optim as opt
import torch.nn.functional as F

import torchvision.datasets as dsets
import torchvision.transforms as transforms

import matplotlib.pyplot as plt
import numpy as np
```



`mnist`的数据在`torchvison`中的`datasets`下，`torchvison`是`pytorch`当中专门用来处理计算机视觉的库。



![](https://moutsea-blog.oss-cn-hangzhou.aliyuncs.com/image-20240407203747760.png)



**下载和引用数据**



```python
dataset_train = dsets.MNIST(root='./data/mnist', train=True, transform=transforms.ToTensor(), download=True)
dataset_test = dsets.MNIST(root='./data/mnist', train=False, transform=transforms.ToTensor())

# 划分验证集
num_train = int(len(dataset_train) * 0.9)  
num_val = len(dataset_train) - num_train

dataset_train, dataset_val = torch.utils.data.random_split(dataset_train, [num_train, num_val])

# 获取dataloader
from torch.utils.data import DataLoader

train_loader = DataLoader(dataset_train, batch_size=64, shuffle=True)
val_loader = DataLoader(dataset_val, batch_size=64, shuffle=True)
test_loader = DataLoader(dataset_test, batch_size=64, shuffle=True)
```



我们可以使用`plt.imshow`函数来将一个二维或三维数组转化成图像：



![](https://moutsea-blog.oss-cn-hangzhou.aliyuncs.com/image-20240407212214294.png)



数据准备妥当，我们就可以来实现模型了。



一张手写数字的样本是一个`28 x 28`的数组，是一个二维数组。这里我们为了简化问题，会将它展开成 `784` 维的一维向量，直接丢入模型当中。



![](https://moutsea-blog.oss-cn-hangzhou.aliyuncs.com/image-20240407212532976.png)



模型部分：



```python
class Net(nn.Module):
    def __init__(self, input_dim, hidden_dim, classes):
        super(Net, self).__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, 64)
        self.fc3 = nn.Linear(64, classes)
        self.relu = nn.ReLU()

    def forward(self, x):
        x = x.view(-1, 28 * 28)
        x = self.relu(self.fc1(x))
        x = self.relu(self.fc2(x))
        # 注意这里的log_softmax
        return torch.log_softmax(self.fc3(x), dim=1)
```



训练部分：



```python
input_dim = 28 * 28
hidden_dim = 128
classes = 10

net = Net(input_dim, hidden_dim, classes)

# 注意这里的NLLLoss
cost = torch.nn.NLLLoss()
optim = torch.optim.Adam(net.parameters(), lr=0.01)
epochs = 10
losses = []

for epoch in range(epochs):
    net.train()

    for x, y in train_loader:
        optim.zero_grad()
        pred = net(x)
        loss = cost(pred, y)
        losses.append(loss.item())
        loss.backward()
        optim.step()

    net.eval()
    with torch.no_grad():
        val_losses = []
        corrects = 0
        for x, y in val_loader:
            pred = net(x)
            _, pval = torch.max(pred, dim=1)
            corrects += (pval == y).sum().item()
            loss = cost(pred, y)
            val_losses.append(loss.item())

    acc = corrects / len(dataset_val)
    print(f'Epoch: {epoch+1}, 训练损失：{np.mean(losses)}, 校验损失：{np.mean(val_losses)}，校验准确率：{acc}')
```



运行之后，我们就得到了训练好的模型，可以看得出来，效果非常不错，准确率高达95%以上。



![](https://moutsea-blog.oss-cn-hangzhou.aliyuncs.com/image-20240407224031664.png)


## log_softmax


注意代码里的细节，我们使用了`log_softmax`作为激活函数，损失函数这里我们用了`NLLLoss`。为什么不是`softmax`和`CrossEntropy`呢？



这里涉及到一个优化。



我们需要基于`softmax`以及交叉熵的公式来理解它，我们首先来看下原版的公式：



![](https://moutsea-blog.oss-cn-hangzhou.aliyuncs.com/image-20240407222927758.png)



我们在计算交叉熵的时候，对$p_i$求了`log`。这里的$p_i$表示的是类别$i$的概率，是一个0到1之间的小数。然而对一个小于1的值求$log$可能存在溢出的问题，我们来看GPT给出的例子：



![](https://moutsea-blog.oss-cn-hangzhou.aliyuncs.com/image-20240407223059393.png)



为了解决这个问题，我们使用`log_softmax`作为激活函数而非`softmax`。它的公式本身很简单，等价于对`softmax`的结果再计算`log`。



![](https://moutsea-blog.oss-cn-hangzhou.aliyuncs.com/image-20240407223226978.png)



我们观察一下交叉熵的公式：$-\sum y_i\log (p_i)$，由于$y_i$是onehot表示的，即只有一维为1，其余均为0。如此一来，这个求和之后的结果就等价于从`log_softmax`中选取$y_i=1$的那一维结果，其余的均丢弃。



而`NLLLoss`损失函数执行的正是这样一个逻辑，因此使用`log_softmax + NLLLoss`要比`softmax + CrossEntropy`的方案效果更好，并且运算效率更高。因为省去了`log`计算的过程，直接查找，当然更快，并且没有溢出的问题，得到的值更精确，覆盖范围更广。



![](https://moutsea-blog.oss-cn-hangzhou.aliyuncs.com/image-20240407223857962.png)



然而这里还有一个小问题，`NLLLoss`当中虽然不再计算$log$了，但`log_softmax`当中不还是要计算$log$吗？只不过是变更了计算顺序而已，好像并没有差别？



其实是有的，因为`log_softmax`虽然等价于`log(softmax)`，但它并不是这么直接计算得到的，而是通过数学变形代换的方法来计算的。具体细节可以参考下图GPT的回答：



![](https://moutsea-blog.oss-cn-hangzhou.aliyuncs.com/image-20240407225730729.png)



我在当前数据集中进行过测试，如果使用`softmax`激活函数，在同样10个epoch训练下，准确率要低好几个百分点，只有90%左右。可见`log_softmax`的确效果更优。



可能很多同学已经很满意于模型的效果了，然而这远远不是模型的极限。实际上我们这种简单粗暴将图片拓展成一维的做法损失了很多空间上的信息，并且拓展性也比较差，那么究竟该怎么做才能效果最优呢？



这部分内容我们放在下一篇文章当中。



最后，宣传一下我的星球。



除了Ai什么都有，目前在更新《穷爸爸与富爸爸》读书笔记，欢迎加入讨论



![](https://moutsea-blog.oss-cn-hangzhou.aliyuncs.com/image-20240404143631289.png)